{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Neural-Tree <p>Neural Search</p> <p></p>"},{"location":"#installation","title":"Installation","text":"<p>We can install neural-tree using:</p> <pre><code>pip install neural-tree\n</code></pre> <p>If we plan to evaluate our model while training install:</p> <pre><code>pip install \"neural-tree[eval]\"\n</code></pre>"},{"location":"api/overview/","title":"Overview","text":""},{"location":"api/overview/#clustering","title":"clustering","text":"<ul> <li>KMeans</li> <li>average</li> <li>get_mapping_nodes_documents</li> <li>optimize_leafs</li> </ul>"},{"location":"api/overview/#datasets","title":"datasets","text":"<ul> <li>load_beir</li> <li>load_beir_test</li> <li>load_beir_train</li> </ul>"},{"location":"api/overview/#leafs","title":"leafs","text":"<ul> <li>Leaf</li> </ul>"},{"location":"api/overview/#nodes","title":"nodes","text":"<ul> <li>Node</li> </ul>"},{"location":"api/overview/#retrievers","title":"retrievers","text":"<ul> <li>ColBERT</li> <li>SentenceTransformer</li> <li>TfIdf</li> </ul>"},{"location":"api/overview/#scoring","title":"scoring","text":"<ul> <li>BaseScore</li> <li>ColBERT</li> <li>SentenceTransformer</li> <li>TfIdf</li> </ul>"},{"location":"api/overview/#trees","title":"trees","text":"<ul> <li>ColBERT</li> <li>SentenceTransformer</li> <li>TfIdf</li> <li>Tree</li> </ul>"},{"location":"api/overview/#utils","title":"utils","text":"<ul> <li>batchify</li> <li>evaluate</li> <li>iter</li> <li>leafs_precision</li> <li>sanity_check</li> <li>set_env</li> </ul>"},{"location":"api/clustering/KMeans/","title":"KMeans","text":"<p>KMeans clustering.</p>"},{"location":"api/clustering/KMeans/#parameters","title":"Parameters","text":"<ul> <li> <p>documents_embeddings (torch.Tensor)</p> </li> <li> <p>n_clusters (int)</p> </li> <li> <p>max_iter (int)</p> </li> <li> <p>n_init (int)</p> </li> <li> <p>seed (int)</p> </li> <li> <p>device (str)</p> </li> </ul>"},{"location":"api/clustering/average/","title":"average","text":"<p>Replace KMeans clustering with average clustering when an existing graph is provided.</p>"},{"location":"api/clustering/average/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> </li> <li> <p>documents (list)</p> </li> <li> <p>documents_embeddings (numpy.ndarray | scipy.sparse._csr.csr_matrix)</p> </li> <li> <p>graph</p> </li> <li> <p>scoring</p> </li> <li> <p>device (str)</p> </li> </ul>"},{"location":"api/clustering/average/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_tree import clustering, scoring\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; documents = [\n...     {\"id\": 0, \"text\": \"Paris is the capital of France.\"},\n...     {\"id\": 1, \"text\": \"Berlin is the capital of Germany.\"},\n...     {\"id\": 2, \"text\": \"Paris and Berlin are European cities.\"},\n...     {\"id\": 3, \"text\": \"Paris and Berlin are beautiful cities.\"},\n... ]\n&gt;&gt;&gt; documents_embeddings = np.array([\n...     [1, 1],\n...     [1, 2],\n...     [10, 10],\n...     [1, 3],\n... ])\n&gt;&gt;&gt; graph = {1: {11: {111: [{'id': 0}, {'id': 3}], 112: [{'id': 1}]}, 12: {121: [{'id': 2}], 122: [{'id': 3}]}}}\n&gt;&gt;&gt; clustering.average(\n...     key=\"id\",\n...     documents_embeddings=documents_embeddings,\n...     documents=documents,\n...     graph=graph[1],\n...     scoring=scoring.SentenceTransformer(key=\"id\", on=[\"text\"], model=None),\n... )\n</code></pre>"},{"location":"api/clustering/get-mapping-nodes-documents/","title":"get_mapping_nodes_documents","text":"<p>Get documents from specific node.</p>"},{"location":"api/clustering/get-mapping-nodes-documents/#parameters","title":"Parameters","text":"<ul> <li> <p>graph (dict | list)</p> </li> <li> <p>documents (list | None) \u2013 defaults to <code>None</code></p> </li> </ul>"},{"location":"api/clustering/optimize-leafs/","title":"optimize_leafs","text":"<p>Optimize the clusters.</p>"},{"location":"api/clustering/optimize-leafs/#parameters","title":"Parameters","text":"<ul> <li> <p>tree</p> </li> <li> <p>documents (list[dict])</p> </li> <li> <p>queries (list[str])</p> </li> <li> <p>k_tree (int) \u2013 defaults to <code>2</code></p> </li> <li> <p>k_retriever (int) \u2013 defaults to <code>10</code></p> </li> <li> <p>k_leafs (int) \u2013 defaults to <code>2</code></p> </li> <li> <p>kwargs</p> </li> </ul>"},{"location":"api/datasets/load-beir-test/","title":"load_beir_test","text":"<p>Load BEIR testing dataset.</p>"},{"location":"api/datasets/load-beir-test/#parameters","title":"Parameters","text":"<ul> <li> <p>dataset_name (str)</p> <p>Dataset name.</p> </li> </ul>"},{"location":"api/datasets/load-beir-test/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_tree import datasets\n&gt;&gt;&gt; documents, queries_ids, queries, qrels = datasets.load_beir_test(\n...     dataset_name=\"scifact\",\n... )\n&gt;&gt;&gt; len(documents)\n5183\n&gt;&gt;&gt; assert len(queries_ids) == len(queries) == len(qrels)\n</code></pre>"},{"location":"api/datasets/load-beir-train/","title":"load_beir_train","text":"<p>Load training dataset.</p>"},{"location":"api/datasets/load-beir-train/#parameters","title":"Parameters","text":"<ul> <li> <p>dataset_name (str)</p> <p>Dataset name</p> </li> </ul>"},{"location":"api/datasets/load-beir-train/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_tree import datasets\n&gt;&gt;&gt; documents, train_queries, train_documents = datasets.load_beir_train(\n...     dataset_name=\"scifact\",\n... )\n&gt;&gt;&gt; len(documents)\n5183\n&gt;&gt;&gt; assert len(train_queries) == len(train_documents)\n</code></pre>"},{"location":"api/datasets/load-beir/","title":"load_beir","text":"<p>Load BEIR dataset.</p>"},{"location":"api/datasets/load-beir/#parameters","title":"Parameters","text":"<ul> <li> <p>dataset_name (str)</p> </li> <li> <p>split (str)</p> </li> </ul>"},{"location":"api/leafs/Leaf/","title":"Leaf","text":"<p>Leaf class.</p>"},{"location":"api/leafs/Leaf/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> </li> <li> <p>level (int)</p> </li> <li> <p>documents (list)</p> </li> <li> <p>documents_embeddings (list)</p> </li> <li> <p>node_name (int)</p> </li> <li> <p>scoring (scoring.TfIdf | scoring.SentenceTransformer)</p> </li> <li> <p>parent (int) \u2013 defaults to <code>0</code></p> </li> <li> <p>create_retrievers (bool) \u2013 defaults to <code>True</code></p> </li> <li> <p>kwargs</p> </li> </ul>"},{"location":"api/leafs/Leaf/#methods","title":"Methods","text":"call <p>Return scores between query and leaf documents.</p> <p>Parameters</p> <ul> <li>queries_embeddings </li> <li>k     (int)    </li> </ul> add <p>Add document to the leaf.</p> <p>Parameters</p> <ul> <li>scoring     (neural_tree.scoring.sentence_transformer.SentenceTransformer | neural_tree.scoring.tfidf.TfIdf)    </li> <li>documents     (list)    </li> <li>documents_embeddings     (dict | None)     \u2013 defaults to <code>None</code> </li> </ul> add_module <p>Adds a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> apply <p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).</p> <p>Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Returns an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> children <p>Returns an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> cpu <p>Moves all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Moves all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> eval <p>Sets the module in evaluation mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> extra_repr <p>Set the extra representation of the module</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.  .. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> <p>Parameters</p> <ul> <li>input     (Any)    </li> </ul> get_buffer <p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_extra_state <p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.</p> <p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_parameter <p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_submodule <p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> ipu <p>Moves all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> load_state_dict <p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p> <p>Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * missing_keys is a list of str containing the missing keys         * unexpected_keys is a list of str containing the unexpected keys  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (Mapping[str, Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> </ul> modules <p>Returns an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> named_buffers <p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[Set[ForwardRef('Module')]])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> nodes_scores <p>Compute the scores between the queries and the leaf.</p> <p>Parameters</p> <ul> <li>scoring     (neural_tree.scoring.sentence_transformer.SentenceTransformer | neural_tree.scoring.tfidf.TfIdf)    </li> <li>queries_embeddings     (torch.Tensor)    </li> <li>node_embedding     (torch.Tensor)    </li> </ul> parameters <p>Returns an iterator over module parameters.</p> <p>This is typically passed to an optimizer.  Args:     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.  Yields:     Parameter: module parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for param in model.parameters():     &gt;&gt;&gt;     print(type(param), param.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_backward_hook <p>Registers a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Adds a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Registers a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Registers a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Registers a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Registers a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; Tensor or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Registers a post hook to be run after module's <code>load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Adds a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_pre_hook <p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> search <p>Return the documents in the leaf.</p> <p>Parameters</p> <ul> <li>tree_scores     (collections.defaultdict)    </li> <li>kwargs </li> </ul> set_extra_state <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p> <p>Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code></p> state_dict <p>Returns a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> to <p>Moves and/or casts the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> to_empty <p>Moves the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[str, torch.device])    </li> </ul> to_json <p>Return the leaf as a json.</p> train <p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Moves all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> zero_grad <p>Sets gradients of all model parameters to zero. See similar function under :class:<code>torch.optim.Optimizer</code> for more context.</p> <p>Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/nodes/Node/","title":"Node","text":"<p>Node of the tree.</p>"},{"location":"api/nodes/Node/#parameters","title":"Parameters","text":"<ul> <li> <p>level (int)</p> </li> <li> <p>key (str)</p> </li> <li> <p>documents_embeddings (torch.Tensor)</p> </li> <li> <p>documents (list)</p> </li> <li> <p>leaf_balance_factor (int)</p> </li> <li> <p>branch_balance_factor (int)</p> </li> <li> <p>device (str)</p> </li> <li> <p>node_name (int | str)</p> </li> <li> <p>scoring (scoring.SentenceTransformer | scoring.TfIdf)</p> </li> <li> <p>seed (int)</p> </li> <li> <p>max_iter (int)</p> </li> <li> <p>n_init (int)</p> </li> <li> <p>parent (int)</p> </li> <li> <p>n_jobs (int)</p> </li> <li> <p>create_retrievers (bool)</p> </li> <li> <p>graph (dict | None)</p> </li> </ul>"},{"location":"api/nodes/Node/#methods","title":"Methods","text":"call <p>Call self as a function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> add_module <p>Adds a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> apply <p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).</p> <p>Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Returns an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> children <p>Returns an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> cpu <p>Moves all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> create_child <p>Create a child.</p> <p>Parameters</p> <ul> <li>level     (int)    </li> <li>node_name     (str)    </li> <li>key     (str)    </li> <li>documents     (list)    </li> <li>documents_embeddings     (list)    </li> <li>scoring     (neural_tree.scoring.sentence_transformer.SentenceTransformer | neural_tree.scoring.tfidf.TfIdf)    </li> <li>max_iter     (int)    </li> <li>n_init     (int)    </li> <li>create_retrievers     (bool)    </li> <li>graph     (dict | list | None)    </li> <li>n_jobs     (int)    </li> <li>seed     (int)    </li> </ul> cuda <p>Moves all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> eval <p>Sets the module in evaluation mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> extra_repr <p>Set the extra representation of the module</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.  .. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> <p>Parameters</p> <ul> <li>input     (Any)    </li> </ul> get_buffer <p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_childs_and_scores <p>Return the childs and scores given matrix of scores.</p> <p>Parameters</p> <ul> <li>queries     (list)    </li> <li>scores     (torch.Tensor)    </li> <li>tree_scores     (collections.defaultdict)    </li> <li>paths     (list | None)    </li> <li>k     (int)    </li> </ul> get_extra_state <p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.</p> <p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_parameter <p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_submodule <p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> ipu <p>Moves all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> load_state_dict <p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p> <p>Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * missing_keys is a list of str containing the missing keys         * unexpected_keys is a list of str containing the unexpected keys  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (Mapping[str, Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> </ul> modules <p>Returns an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> named_buffers <p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[Set[ForwardRef('Module')]])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> nodes_scores <p>Return the scores of the embeddings.</p> <p>Parameters</p> <ul> <li>scoring     (neural_tree.scoring.sentence_transformer.SentenceTransformer | neural_tree.scoring.tfidf.TfIdf)    </li> <li>queries_embeddings     (torch.Tensor)    </li> <li>kwargs </li> </ul> parameters <p>Returns an iterator over module parameters.</p> <p>This is typically passed to an optimizer.  Args:     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.  Yields:     Parameter: module parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for param in model.parameters():     &gt;&gt;&gt;     print(type(param), param.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_backward_hook <p>Registers a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Adds a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Registers a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Registers a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Registers a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Registers a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; Tensor or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Registers a post hook to be run after module's <code>load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Adds a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_pre_hook <p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> search <p>Search for the closest embedding.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>queries_embeddings     (torch.Tensor)    </li> <li>scoring     (neural_tree.scoring.sentence_transformer.SentenceTransformer | neural_tree.scoring.tfidf.TfIdf)    </li> <li>k     (int)    </li> <li>beam_search_depth     (int)    </li> <li>paths     (dict[list] | None)     \u2013 defaults to <code>None</code> </li> <li>tree_scores     (collections.defaultdict | None)     \u2013 defaults to <code>None</code> </li> <li>kwargs </li> </ul> set_extra_state <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p> <p>Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code></p> state_dict <p>Returns a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> to <p>Moves and/or casts the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> to_empty <p>Moves the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[str, torch.device])    </li> </ul> to_json train <p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Moves all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> zero_grad <p>Sets gradients of all model parameters to zero. See similar function under :class:<code>torch.optim.Optimizer</code> for more context.</p> <p>Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/retrievers/ColBERT/","title":"ColBERT","text":"<p>ColBERT retriever.</p>"},{"location":"api/retrievers/ColBERT/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> </li> <li> <p>on (str | list[str])</p> </li> <li> <p>device (str)</p> </li> </ul>"},{"location":"api/retrievers/ColBERT/#methods","title":"Methods","text":"call <p>Rank documents  givent queries.</p> <p>Parameters</p> <ul> <li>queries_embeddings     (dict[str, torch.Tensor])    </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>k     (int)     \u2013 defaults to <code>None</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>False</code> </li> </ul> add <p>Add documents embeddings.</p> <p>Parameters</p> <ul> <li>documents_embeddings     (dict)    </li> </ul> encode_documents <p>Encode documents.</p> <p>Parameters</p> <ul> <li>documents     (list[str])    </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>query_mode     (bool)     \u2013 defaults to <code>False</code> </li> <li>kwargs </li> </ul> encode_queries <p>Encode queries.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> <li>query_mode     (bool)     \u2013 defaults to <code>True</code> </li> <li>kwargs </li> </ul>"},{"location":"api/retrievers/SentenceTransformer/","title":"SentenceTransformer","text":"<p>Sentence Transformer retriever.</p>"},{"location":"api/retrievers/SentenceTransformer/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> </li> <li> <p>device (str) \u2013 defaults to <code>cpu</code></p> </li> </ul>"},{"location":"api/retrievers/SentenceTransformer/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_tree import retrievers\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; model = SentenceTransformer(\"all-mpnet-base-v2\")\n&gt;&gt;&gt; retriever = retrievers.SentenceTransformer(key=\"id\")\n&gt;&gt;&gt; retriever = retriever.add(\n...     documents_embeddings={\n...         0: model.encode(\"Paris is the capital of France.\"),\n...         1: model.encode(\"Berlin is the capital of Germany.\"),\n...         2: model.encode(\"Paris and Berlin are European cities.\"),\n...         3: model.encode(\"Paris and Berlin are beautiful cities.\"),\n...     }\n... )\n&gt;&gt;&gt; queries_embeddings = {\n...     0: model.encode(\"Paris\"),\n...     1: model.encode(\"Berlin\"),\n... }\n&gt;&gt;&gt; candidates = retriever(queries_embeddings=queries_embeddings, k=2)\n&gt;&gt;&gt; pprint(candidates)\n[[{'id': 0, 'similarity': 0.644777984318611},\n{'id': 3, 'similarity': 0.52865785276988}],\n[{'id': 1, 'similarity': 0.6901492368348436},\n{'id': 3, 'similarity': 0.5457692206973245}]]\n</code></pre>"},{"location":"api/retrievers/SentenceTransformer/#methods","title":"Methods","text":"call <p>Retrieve documents.</p> <p>Parameters</p> <ul> <li>queries_embeddings     (dict[int, numpy.ndarray])    </li> <li>k     (int | None)     \u2013 defaults to <code>100</code> </li> <li>kwargs </li> </ul> add <p>Add documents to the faiss index.</p> <p>Parameters</p> <ul> <li>documents_embeddings     (dict[int, numpy.ndarray])    </li> </ul>"},{"location":"api/retrievers/TfIdf/","title":"TfIdf","text":"<p>TfIdf retriever</p>"},{"location":"api/retrievers/TfIdf/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> </li> <li> <p>on (list[str])</p> </li> </ul>"},{"location":"api/retrievers/TfIdf/#methods","title":"Methods","text":"call <p>Retrieve documents from batch of queries.</p> <p>Parameters</p> <ul> <li>queries_embeddings     (dict[str, scipy.sparse._csr.csr_matrix])    </li> <li>k     (int)     \u2013 defaults to <code>None</code> </li> <li>batch_size     (int)     \u2013 defaults to <code>2000</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> </ul> add <p>Add new documents to the TFIDF retriever. The tfidf won't be refitted.</p> <p>Parameters</p> <ul> <li>documents_embeddings     (dict[str, scipy.sparse._csr.csr_matrix])    </li> </ul> encode_documents <p>Encode queries into sparse matrix.</p> <p>Parameters</p> <ul> <li>documents     (list[dict])    </li> <li>model     (sklearn.feature_extraction.text.TfidfVectorizer)    </li> </ul> encode_queries <p>Encode queries into sparse matrix.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>model     (sklearn.feature_extraction.text.TfidfVectorizer)    </li> </ul> top_k <p>Return the top k documents for each query.</p> <p>Parameters</p> <ul> <li>similarities     (scipy.sparse._csc.csc_matrix)    </li> <li>k     (int)    </li> </ul>"},{"location":"api/scoring/BaseScore/","title":"BaseScore","text":"<p>Base class for scoring functions.</p>"},{"location":"api/scoring/BaseScore/#methods","title":"Methods","text":"convert_to_tensor <p>Transform sparse matrix to tensor.</p> <p>Parameters</p> <ul> <li>embeddings     (scipy.sparse._csr.csr_matrix | numpy.ndarray)    </li> <li>device     (str)    </li> </ul> distinct_documents_encoder <p>Return True if the encoder is distinct for documents and nodes.</p> encode_queries_for_retrieval <p>Encode queries for retrieval.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> </ul> get_retriever <p>Create a retriever</p> leaf_scores <p>Return the scores of the embeddings.</p> <p>Parameters</p> <ul> <li>queries_embeddings     (torch.Tensor)    </li> <li>leaf_embedding     (torch.Tensor)    </li> </ul> nodes_scores <p>Score between queries and nodes embeddings.</p> <p>Parameters</p> <ul> <li>queries_embeddings     (torch.Tensor)    </li> <li>nodes_embeddings     (torch.Tensor)    </li> </ul> stack <p>Stack list of embeddings.</p> <ul> <li>embeddings     (list[scipy.sparse._csr.csr_matrix | numpy.ndarray | dict])    </li> </ul> transform_documents <p>Transform documents to embeddings.</p> <p>Parameters</p> <ul> <li>documents     (list[dict])    </li> </ul> transform_queries <p>Transform queries to embeddings.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> </ul>"},{"location":"api/scoring/ColBERT/","title":"ColBERT","text":"<p>TfIdf scoring function.</p>"},{"location":"api/scoring/ColBERT/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> </li> <li> <p>on (list | str)</p> </li> <li> <p>documents (list)</p> </li> <li> <p>model (neural_cherche.models.colbert.ColBERT) \u2013 defaults to <code>None</code></p> </li> <li> <p>device (str) \u2013 defaults to <code>cpu</code></p> </li> <li> <p>kwargs</p> </li> </ul>"},{"location":"api/scoring/ColBERT/#attributes","title":"Attributes","text":"<ul> <li> <p>distinct_documents_encoder</p> <p>Return True if the encoder is distinct for documents and nodes.</p> </li> </ul>"},{"location":"api/scoring/ColBERT/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_tree import trees, scoring\n&gt;&gt;&gt; from neural_cherche import models\n&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer\n&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; _ = torch.manual_seed(42)\n&gt;&gt;&gt; documents = [\n...     {\"id\": 0, \"text\": \"Paris is the capital of France.\"},\n...     {\"id\": 1, \"text\": \"Berlin is the capital of Germany.\"},\n...     {\"id\": 2, \"text\": \"Paris and Berlin are European cities.\"},\n...     {\"id\": 3, \"text\": \"Paris and Berlin are beautiful cities.\"},\n... ]\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-mpnet-base-v2\",\n...     embedding_size=128,\n...     max_length_document=96,\n...     max_length_query=32,\n... )\n&gt;&gt;&gt; tree = trees.ColBERTTree(\n...    key=\"id\",\n...    on=\"text\",\n...    model=model,\n...    documents=documents,\n...    leaf_balance_factor=1,\n...    branch_balance_factor=2,\n...    n_jobs=1,\n... )\n&gt;&gt;&gt; print(tree)\nnode 1\nnode 10\nleaf 100\nleaf 101\nnode 11\nleaf 110\nleaf 111\n&gt;&gt;&gt; tree.leafs_to_documents\n{'100': [0], '101': [1], '110': [2], '111': [3]}\n&gt;&gt;&gt; candidates = tree(\n...    queries=[\"Paris is the capital of France.\", \"Paris and Berlin are European cities.\"],\n...    k_leafs=2,\n...    k=2,\n... )\n&gt;&gt;&gt; candidates[\"scores\"]\narray([[28.12037659, 18.32332611],\n[29.28324509, 21.38923264]])\n&gt;&gt;&gt; candidates[\"leafs\"]\narray([['100', '101'],\n['110', '111']], dtype='&lt;U3')\n&gt;&gt;&gt; pprint(candidates[\"tree_scores\"])\n[{'10': tensor(28.1204),\n'100': tensor(28.1204),\n'101': tensor(18.3233),\n'11': tensor(20.9327)},\n{'10': tensor(21.6886),\n'11': tensor(29.2832),\n'110': tensor(29.2832),\n'111': tensor(21.3892)}]\n&gt;&gt;&gt; pprint(candidates[\"documents\"])\n[[{'id': 0, 'leaf': '100', 'similarity': 28.120376586914062},\n{'id': 1, 'leaf': '101', 'similarity': 18.323326110839844}],\n[{'id': 2, 'leaf': '110', 'similarity': 29.283245086669922},\n{'id': 3, 'leaf': '111', 'similarity': 21.389232635498047}]]\n</code></pre>"},{"location":"api/scoring/ColBERT/#methods","title":"Methods","text":"average <p>Average embeddings.</p> <ul> <li>embeddings     (torch.Tensor)    </li> </ul> convert_to_tensor <p>Transform sparse matrix to tensor.</p> <p>Parameters</p> <ul> <li>embeddings     (numpy.ndarray | torch.Tensor)    </li> <li>device     (str)    </li> </ul> encode_queries_for_retrieval <p>Encode queries for retrieval.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> </ul> get_retriever <p>Create a retriever</p> leaf_scores <p>Return the scores of the embeddings.</p> <p>Parameters</p> <ul> <li>queries_embeddings     (torch.Tensor)    </li> <li>leaf_embedding     (torch.Tensor)    </li> </ul> nodes_scores <p>Score between queries and nodes embeddings.</p> <p>Parameters</p> <ul> <li>queries_embeddings     (torch.Tensor)    </li> <li>nodes_embeddings     (torch.Tensor)    </li> </ul> stack <p>Stack list of embeddings.</p> <p>Parameters</p> <ul> <li>embeddings     (list[torch.Tensor | numpy.ndarray])    </li> </ul> transform_documents <p>Transform documents to embeddings.</p> <p>Parameters</p> <ul> <li>documents     (list[dict])    </li> <li>batch_size     (int)    </li> <li>tqdm_bar     (bool)    </li> <li>kwargs </li> </ul> transform_queries <p>Transform queries to embeddings.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>batch_size     (int)    </li> <li>tqdm_bar     (bool)    </li> <li>kwargs </li> </ul>"},{"location":"api/scoring/SentenceTransformer/","title":"SentenceTransformer","text":"<p>Sentence Transformer scoring function.</p>"},{"location":"api/scoring/SentenceTransformer/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> </li> <li> <p>on (str | list)</p> </li> <li> <p>model (sentence_transformers.SentenceTransformer.SentenceTransformer)</p> </li> <li> <p>device (str) \u2013 defaults to <code>cpu</code></p> </li> <li> <p>faiss_device (str) \u2013 defaults to <code>cpu</code></p> </li> </ul>"},{"location":"api/scoring/SentenceTransformer/#attributes","title":"Attributes","text":"<ul> <li> <p>distinct_documents_encoder</p> <p>Return True if the encoder is distinct for documents and nodes.</p> </li> </ul>"},{"location":"api/scoring/SentenceTransformer/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_tree import trees, scoring\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; documents = [\n...     {\"id\": 0, \"text\": \"Paris is the capital of France.\"},\n...     {\"id\": 1, \"text\": \"Berlin is the capital of Germany.\"},\n...     {\"id\": 2, \"text\": \"Paris and Berlin are European cities.\"},\n...     {\"id\": 3, \"text\": \"Paris and Berlin are beautiful cities.\"},\n... ]\n&gt;&gt;&gt; tree = trees.Tree(\n...    key=\"id\",\n...    documents=documents,\n...    scoring=scoring.SentenceTransformer(key=\"id\", on=[\"text\"], model=SentenceTransformer(\"all-mpnet-base-v2\")),\n...    leaf_balance_factor=1,\n...    branch_balance_factor=2,\n...    n_jobs=1,\n... )\n&gt;&gt;&gt; print(tree)\nnode 1\nnode 11\nnode 110\nleaf 1100\nleaf 1101\nleaf 111\nleaf 10\n&gt;&gt;&gt; candidates = tree(\n...    queries=[\"paris\", \"berlin\"],\n...    k_leafs=2,\n... )\n&gt;&gt;&gt; candidates[\"scores\"]\narray([[0.72453916, 0.60635257],\n[0.58386189, 0.57546711]])\n&gt;&gt;&gt; candidates[\"leafs\"]\narray([['111', '10'],\n['1101', '1100']], dtype='&lt;U4')\n&gt;&gt;&gt; pprint(candidates[\"tree_scores\"])\n[{'10': tensor(0.6064),\n'11': tensor(0.7245),\n'110': tensor(0.5542),\n'1100': tensor(0.5403),\n'1101': tensor(0.5542),\n'111': tensor(0.7245)},\n{'10': tensor(0.5206),\n'11': tensor(0.5797),\n'110': tensor(0.5839),\n'1100': tensor(0.5755),\n'1101': tensor(0.5839),\n'111': tensor(0.4026)}]\n&gt;&gt;&gt; pprint(candidates[\"documents\"])\n[[{'id': 0, 'leaf': '111', 'similarity': 0.6447779347587058},\n{'id': 1, 'leaf': '10', 'similarity': 0.43175890864117644}],\n[{'id': 3, 'leaf': '1101', 'similarity': 0.545769273959571},\n{'id': 2, 'leaf': '1100', 'similarity': 0.54081365990618}]]\n</code></pre>"},{"location":"api/scoring/SentenceTransformer/#methods","title":"Methods","text":"average <p>Average embeddings.</p> <ul> <li>embeddings     (numpy.ndarray)    </li> </ul> convert_to_tensor <p>Convert numpy array to torch tensor.</p> <p>Parameters</p> <ul> <li>embeddings     (numpy.ndarray)    </li> <li>device     (str)    </li> </ul> encode_queries_for_retrieval <p>Encode queries for retrieval.</p> <ul> <li>queries     (list[str])    </li> </ul> get_retriever <p>Create a retriever</p> leaf_scores <p>Computes scores between query and leaf embedding.</p> <p>Parameters</p> <ul> <li>queries_embeddings     (torch.Tensor)    </li> <li>leaf_embedding     (torch.Tensor)    </li> </ul> nodes_scores <p>Score between queries and nodes embeddings.</p> <p>Parameters</p> <ul> <li>queries_embeddings     (torch.Tensor)    </li> <li>nodes_embeddings     (torch.Tensor)    </li> </ul> stack <p>Stack embeddings.</p> <ul> <li>embeddings     (list[numpy.ndarray])    </li> </ul> transform_documents <p>Transform documents to embeddings.</p> <p>Parameters</p> <ul> <li>documents     (list[dict])    </li> <li>batch_size     (int)    </li> <li>kwargs </li> </ul> transform_queries <p>Transform queries to embeddings.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>batch_size     (int)    </li> <li>kwargs </li> </ul>"},{"location":"api/scoring/TfIdf/","title":"TfIdf","text":"<p>TfIdf scoring function.</p>"},{"location":"api/scoring/TfIdf/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> </li> <li> <p>on (list | str)</p> </li> <li> <p>documents (list)</p> </li> <li> <p>tfidf_nodes (sklearn.feature_extraction.text.TfidfVectorizer | None) \u2013 defaults to <code>None</code></p> </li> <li> <p>tfidf_documents (sklearn.feature_extraction.text.TfidfVectorizer | None) \u2013 defaults to <code>None</code></p> </li> <li> <p>kwargs</p> </li> </ul>"},{"location":"api/scoring/TfIdf/#attributes","title":"Attributes","text":"<ul> <li> <p>distinct_documents_encoder</p> <p>Return True if the encoder is distinct for documents and nodes.</p> </li> </ul>"},{"location":"api/scoring/TfIdf/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_tree import trees, scoring\n&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer\n&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; documents = [\n...     {\"id\": 0, \"text\": \"Paris is the capital of France.\"},\n...     {\"id\": 1, \"text\": \"Berlin is the capital of Germany.\"},\n...     {\"id\": 2, \"text\": \"Paris and Berlin are European cities.\"},\n...     {\"id\": 3, \"text\": \"Paris and Berlin are beautiful cities.\"},\n... ]\n&gt;&gt;&gt; tree = trees.Tree(\n...    key=\"id\",\n...    documents=documents,\n...    scoring=scoring.TfIdf(key=\"id\", on=[\"text\"], documents=documents),\n...    leaf_balance_factor=1,\n...    branch_balance_factor=2,\n... )\n&gt;&gt;&gt; print(tree)\nnode 1\nnode 10\nleaf 100\nleaf 101\nnode 11\nleaf 110\nleaf 111\n&gt;&gt;&gt; tree.leafs_to_documents\n{'100': [0], '101': [1], '110': [2], '111': [3]}\n&gt;&gt;&gt; candidates = tree(\n...    queries=[\"Paris is the capital of France.\", \"Paris and Berlin are European cities.\"],\n...    k_leafs=2,\n...    k=2,\n... )\n&gt;&gt;&gt; candidates[\"scores\"]\narray([[0.99999994, 0.63854915],\n[0.99999994, 0.72823119]])\n&gt;&gt;&gt; candidates[\"leafs\"]\narray([['100', '101'],\n['110', '111']], dtype='&lt;U3')\n&gt;&gt;&gt; pprint(candidates[\"tree_scores\"])\n[{'10': tensor(1.0000),\n'100': tensor(1.0000),\n'101': tensor(0.6385),\n'11': tensor(0.1076)},\n{'10': tensor(0.1076),\n'11': tensor(1.0000),\n'110': tensor(1.0000),\n'111': tensor(0.7282)}]\n&gt;&gt;&gt; pprint(candidates[\"documents\"])\n[[{'id': 0, 'leaf': '100', 'similarity': 0.9999999999999978},\n{'id': 1, 'leaf': '101', 'similarity': 0.39941742405759667}],\n[{'id': 2, 'leaf': '110', 'similarity': 0.9999999999999978},\n{'id': 3, 'leaf': '111', 'similarity': 0.5385719658738707}]]\n</code></pre>"},{"location":"api/scoring/TfIdf/#methods","title":"Methods","text":"average <p>Average embeddings.</p> <ul> <li>embeddings     (scipy.sparse._csr.csr_matrix)    </li> </ul> convert_to_tensor <p>Transform sparse matrix to tensor.</p> <p>Parameters</p> <ul> <li>embeddings     (scipy.sparse._csr.csr_matrix)    </li> <li>device     (str)    </li> </ul> encode_queries_for_retrieval <p>Encode queries for retrieval.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> </ul> get_retriever <p>Create a retriever</p> leaf_scores <p>Return the scores of the embeddings.</p> <p>Parameters</p> <ul> <li>queries_embeddings     (torch.Tensor)    </li> <li>leaf_embedding     (torch.Tensor)    </li> </ul> nodes_scores <p>Score between queries and nodes embeddings.</p> <p>Parameters</p> <ul> <li>queries_embeddings     (torch.Tensor)    </li> <li>nodes_embeddings     (torch.Tensor)    </li> </ul> stack <p>Stack list of embeddings.</p> <ul> <li>embeddings     (list[scipy.sparse._csr.csr_matrix])    </li> </ul> transform_documents <p>Transform documents to embeddings.</p> <p>Parameters</p> <ul> <li>documents     (list[dict])    </li> <li>kwargs </li> </ul> transform_queries <p>Transform queries to embeddings.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>kwargs </li> </ul>"},{"location":"api/trees/ColBERT/","title":"ColBERT","text":""},{"location":"api/trees/ColBERT/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> </li> <li> <p>on (str | list[str])</p> </li> <li> <p>model (neural_cherche.models.colbert.ColBERT)</p> </li> <li> <p>sentence_transformer (sentence_transformers.SentenceTransformer.SentenceTransformer | None) \u2013 defaults to <code>None</code></p> </li> <li> <p>documents (list[dict] | None) \u2013 defaults to <code>None</code></p> </li> <li> <p>graph (dict | None) \u2013 defaults to <code>None</code></p> </li> <li> <p>leaf_balance_factor (int) \u2013 defaults to <code>100</code></p> </li> <li> <p>branch_balance_factor (int) \u2013 defaults to <code>5</code></p> </li> <li> <p>device (str) \u2013 defaults to <code>cpu</code></p> </li> <li> <p>n_jobs (int) \u2013 defaults to <code>-1</code></p> </li> <li> <p>batch_size (int) \u2013 defaults to <code>32</code></p> </li> <li> <p>max_iter (int) \u2013 defaults to <code>3000</code></p> </li> <li> <p>n_init (int) \u2013 defaults to <code>100</code></p> </li> <li> <p>create_retrievers (bool) \u2013 defaults to <code>True</code></p> </li> <li> <p>tqdm_bar (bool) \u2013 defaults to <code>True</code></p> </li> <li> <p>seed (int) \u2013 defaults to <code>42</code></p> </li> </ul>"},{"location":"api/trees/ColBERT/#methods","title":"Methods","text":"call <p>Search for the closest embedding.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>k     (int)     \u2013 defaults to <code>100</code> </li> <li>k_leafs     (int)     \u2013 defaults to <code>1</code> </li> <li>leafs     (list[int] | None)     \u2013 defaults to <code>None</code> </li> <li>score_documents     (bool)     \u2013 defaults to <code>True</code> </li> <li>beam_search_depth     (int)     \u2013 defaults to <code>1</code> </li> <li>queries_embeddings     (torch.Tensor | numpy.ndarray | dict)     \u2013 defaults to <code>None</code> </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> </ul> add <p>Add documents to the tree.</p> <p>Parameters</p> <ul> <li>documents     (list)    </li> <li>documents_embeddings     (numpy.ndarray | scipy.sparse._csr.csr_matrix | dict)     \u2013 defaults to <code>None</code> </li> <li>k     (int)     \u2013 defaults to <code>1</code> </li> <li>documents_to_leafs     (dict)     \u2013 defaults to <code>None</code> </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> </ul> add_module <p>Adds a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> apply <p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).</p> <p>Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Returns an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> children <p>Returns an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> cpu <p>Moves all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Moves all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> empty <p>Empty the tree.</p> eval <p>Sets the module in evaluation mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> extra_repr <p>Set the extra representation of the module</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.  .. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> <p>Parameters</p> <ul> <li>input     (Any)    </li> </ul> get_buffer <p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_documents_leafs <p>Returns mapping between documents ids and leafs and vice versa.</p> get_extra_state <p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.</p> <p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_mapping_leafs <p>Returns mapping between leafs and their number.</p> get_negative_samples <p>Return negative samples build from the tree.</p> get_parameter <p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_parent <p>Get parent nodes of a specifc node.</p> <p>Parameters</p> <ul> <li>node_name     (int | str)    </li> </ul> get_paths <p>Map leafs to their nodes.</p> get_submodule <p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> ipu <p>Moves all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> load_state_dict <p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p> <p>Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * missing_keys is a list of str containing the missing keys         * unexpected_keys is a list of str containing the unexpected keys  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (Mapping[str, Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> </ul> loss <p>Computes the loss of the tree given the input batch.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>documents     (list[dict])    </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> </ul> modules <p>Returns an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> named_buffers <p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[Set[ForwardRef('Module')]])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> nodes <p>Iterate over the nodes of the tree.</p> <p>Parameters</p> <ul> <li>node     (neural_tree.nodes.node.Node | neural_tree.leafs.leaf.Leaf)     \u2013 defaults to <code>None</code> </li> </ul> parameters <p>Return the parameters of the tree.</p> paths register_backward_hook <p>Registers a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Adds a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Registers a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Registers a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Registers a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Registers a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; Tensor or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Registers a post hook to be run after module's <code>load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Adds a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_pre_hook <p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> set_extra_state <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p> <p>Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code></p> state_dict <p>Returns a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> to <p>Moves and/or casts the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> to_empty <p>Moves the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[str, torch.device])    </li> </ul> to_json <p>Return the tree as a graph.</p> train <p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Moves all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> zero_grad <p>Sets gradients of all model parameters to zero. See similar function under :class:<code>torch.optim.Optimizer</code> for more context.</p> <p>Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/trees/SentenceTransformer/","title":"SentenceTransformer","text":"<p>Tree with Sentence Transformer scoring function.</p>"},{"location":"api/trees/SentenceTransformer/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> </li> <li> <p>on (str | list[str])</p> </li> <li> <p>model (sentence_transformers.SentenceTransformer.SentenceTransformer)</p> </li> <li> <p>documents (list[dict] | None) \u2013 defaults to <code>None</code></p> </li> <li> <p>documents_embeddings (dict | None) \u2013 defaults to <code>None</code></p> </li> <li> <p>graph (dict | None) \u2013 defaults to <code>None</code></p> </li> <li> <p>leaf_balance_factor (int) \u2013 defaults to <code>100</code></p> </li> <li> <p>branch_balance_factor (int) \u2013 defaults to <code>5</code></p> </li> <li> <p>device (str) \u2013 defaults to <code>cpu</code></p> </li> <li> <p>faiss_device (str) \u2013 defaults to <code>cpu</code></p> </li> <li> <p>batch_size (int) \u2013 defaults to <code>32</code></p> </li> <li> <p>n_jobs (int) \u2013 defaults to <code>-1</code></p> </li> <li> <p>max_iter (int) \u2013 defaults to <code>3000</code></p> </li> <li> <p>n_init (int) \u2013 defaults to <code>100</code></p> </li> <li> <p>create_retrievers (bool) \u2013 defaults to <code>True</code></p> </li> <li> <p>seed (int) \u2013 defaults to <code>42</code></p> </li> </ul>"},{"location":"api/trees/SentenceTransformer/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_tree import trees\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; documents = [\n...     {\"id\": 0, \"text\": \"Paris is the capital of France.\"},\n...     {\"id\": 1, \"text\": \"Berlin is the capital of Germany.\"},\n...     {\"id\": 2, \"text\": \"Paris and Berlin are European cities.\"},\n...     {\"id\": 3, \"text\": \"Paris and Berlin are beautiful cities.\"},\n... ]\n&gt;&gt;&gt; tree = trees.SentenceTransformer(\n...    key=\"id\",\n...    on=[\"text\"],\n...    documents=documents,\n...    model=SentenceTransformer(\"all-mpnet-base-v2\"),\n...    leaf_balance_factor=2,\n...    branch_balance_factor=2,\n...    device=\"cpu\",\n... )\n&gt;&gt;&gt; tree = tree.add(documents=documents)\n&gt;&gt;&gt; print(tree)\nnode 1\nnode 11\nleaf 110\nleaf 111\nleaf 10\n&gt;&gt;&gt; tree.leafs_to_documents\n{'110': [2, 3, 1], '111': [0], '10': [1]}\n&gt;&gt;&gt; candidates = tree(\n...    queries=[\"Paris is the capital of France.\", \"Paris and Berlin are European cities.\"],\n...    k_leafs=2,\n...    k=1,\n... )\n&gt;&gt;&gt; candidates[\"scores\"]\narray([[1.        , 0.76908004],\n[0.88792843, 0.82272887]])\n&gt;&gt;&gt; candidates[\"leafs\"]\narray([['111', '10'],\n['110', '10']], dtype='&lt;U3')\n&gt;&gt;&gt; pprint(candidates[\"tree_scores\"])\n[{'10': tensor(0.7691, device='mps:0'),\n'11': tensor(1., device='mps:0'),\n'110': tensor(0.6536, device='mps:0'),\n'111': tensor(1., device='mps:0')},\n{'10': tensor(0.8227, device='mps:0'),\n'11': tensor(0.8879, device='mps:0'),\n'110': tensor(0.8879, device='mps:0'),\n'111': tensor(0.6923, device='mps:0')}]\n&gt;&gt;&gt; pprint(candidates[\"documents\"])\n[[{'id': 0, 'leaf': '111', 'similarity': 1.0}],\n[{'id': 2, 'leaf': '110', 'similarity': 1.0}]]\n</code></pre>"},{"location":"api/trees/SentenceTransformer/#methods","title":"Methods","text":"call <p>Search for the closest embedding.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>k     (int)     \u2013 defaults to <code>100</code> </li> <li>k_leafs     (int)     \u2013 defaults to <code>1</code> </li> <li>leafs     (list[int] | None)     \u2013 defaults to <code>None</code> </li> <li>score_documents     (bool)     \u2013 defaults to <code>True</code> </li> <li>beam_search_depth     (int)     \u2013 defaults to <code>1</code> </li> <li>queries_embeddings     (torch.Tensor | numpy.ndarray | dict)     \u2013 defaults to <code>None</code> </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> </ul> add <p>Add documents to the tree.</p> <p>Parameters</p> <ul> <li>documents     (list)    </li> <li>documents_embeddings     (numpy.ndarray | scipy.sparse._csr.csr_matrix | dict)     \u2013 defaults to <code>None</code> </li> <li>k     (int)     \u2013 defaults to <code>1</code> </li> <li>documents_to_leafs     (dict)     \u2013 defaults to <code>None</code> </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> </ul> add_module <p>Adds a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> apply <p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).</p> <p>Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Returns an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> children <p>Returns an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> cpu <p>Moves all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Moves all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> empty <p>Empty the tree.</p> eval <p>Sets the module in evaluation mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> extra_repr <p>Set the extra representation of the module</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.  .. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> <p>Parameters</p> <ul> <li>input     (Any)    </li> </ul> get_buffer <p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_documents_leafs <p>Returns mapping between documents ids and leafs and vice versa.</p> get_extra_state <p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.</p> <p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_mapping_leafs <p>Returns mapping between leafs and their number.</p> get_negative_samples <p>Return negative samples build from the tree.</p> get_parameter <p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_parent <p>Get parent nodes of a specifc node.</p> <p>Parameters</p> <ul> <li>node_name     (int | str)    </li> </ul> get_paths <p>Map leafs to their nodes.</p> get_submodule <p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> ipu <p>Moves all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> load_state_dict <p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p> <p>Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * missing_keys is a list of str containing the missing keys         * unexpected_keys is a list of str containing the unexpected keys  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (Mapping[str, Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> </ul> loss <p>Computes the loss of the tree given the input batch.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>documents     (list[dict])    </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> </ul> modules <p>Returns an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> named_buffers <p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[Set[ForwardRef('Module')]])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> nodes <p>Iterate over the nodes of the tree.</p> <p>Parameters</p> <ul> <li>node     (neural_tree.nodes.node.Node | neural_tree.leafs.leaf.Leaf)     \u2013 defaults to <code>None</code> </li> </ul> parameters <p>Return the parameters of the tree.</p> paths register_backward_hook <p>Registers a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Adds a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Registers a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Registers a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Registers a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Registers a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; Tensor or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Registers a post hook to be run after module's <code>load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Adds a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_pre_hook <p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> set_extra_state <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p> <p>Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code></p> state_dict <p>Returns a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> to <p>Moves and/or casts the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> to_empty <p>Moves the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[str, torch.device])    </li> </ul> to_json <p>Return the tree as a graph.</p> train <p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Moves all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> zero_grad <p>Sets gradients of all model parameters to zero. See similar function under :class:<code>torch.optim.Optimizer</code> for more context.</p> <p>Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/trees/TfIdf/","title":"TfIdf","text":"<p>Tree with tfidf scoring function.</p>"},{"location":"api/trees/TfIdf/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> </li> <li> <p>on (str | list[str])</p> </li> <li> <p>documents (list[dict] | None) \u2013 defaults to <code>None</code></p> </li> <li> <p>graph (dict | None) \u2013 defaults to <code>None</code></p> </li> <li> <p>leaf_balance_factor (int) \u2013 defaults to <code>100</code></p> </li> <li> <p>branch_balance_factor (int) \u2013 defaults to <code>5</code></p> </li> <li> <p>tfidf_nodes (sklearn.feature_extraction.text.TfidfVectorizer | None) \u2013 defaults to <code>None</code></p> </li> <li> <p>tfidf_documents (sklearn.feature_extraction.text.TfidfVectorizer | None) \u2013 defaults to <code>None</code></p> </li> <li> <p>device (str) \u2013 defaults to <code>cpu</code></p> </li> <li> <p>n_jobs (int) \u2013 defaults to <code>-1</code></p> </li> <li> <p>max_iter (int) \u2013 defaults to <code>3000</code></p> </li> <li> <p>n_init (int) \u2013 defaults to <code>100</code></p> </li> <li> <p>create_retrievers (bool) \u2013 defaults to <code>True</code></p> </li> <li> <p>seed (int) \u2013 defaults to <code>42</code></p> </li> </ul>"},{"location":"api/trees/TfIdf/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_tree import trees\n&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; documents = [\n...     {\"id\": 0, \"text\": \"Paris is the capital of France.\"},\n...     {\"id\": 1, \"text\": \"Berlin is the capital of Germany.\"},\n...     {\"id\": 2, \"text\": \"Paris and Berlin are European cities.\"},\n...     {\"id\": 3, \"text\": \"Paris and Berlin are beautiful cities.\"},\n... ]\n&gt;&gt;&gt; tree = trees.TfIdf(\n...    key=\"id\",\n...    on=\"text\",\n...    documents=documents,\n...    leaf_balance_factor=2,\n...    branch_balance_factor=2,\n... )\n&gt;&gt;&gt; tree = tree.add(documents=documents)\n&gt;&gt;&gt; print(tree)\nnode 1\nleaf 10\nleaf 11\n&gt;&gt;&gt; tree.leafs_to_documents\n{'10': [0, 1], '11': [2, 3]}\n&gt;&gt;&gt; candidates = tree(\n...    queries=[\"Paris is the capital of France.\", \"Paris and Berlin are European cities.\"],\n...    k_leafs=2,\n...    k=2,\n... )\n&gt;&gt;&gt; candidates[\"scores\"]\narray([[0.81927449, 0.10763316],\n[0.8641156 , 0.10763316]])\n&gt;&gt;&gt; candidates[\"leafs\"]\narray([['10', '11'],\n['11', '10']], dtype='&lt;U2')\n&gt;&gt;&gt; pprint(candidates[\"tree_scores\"])\n[{'10': tensor(0.8193), '11': tensor(0.1076)},\n{'10': tensor(0.1076), '11': tensor(0.8641)}]\n&gt;&gt;&gt; pprint(candidates[\"documents\"])\n[[{'id': 0, 'leaf': '10', 'similarity': 0.9999999999999978},\n{'id': 1, 'leaf': '10', 'similarity': 0.39941742405759667}],\n[{'id': 2, 'leaf': '11', 'similarity': 0.9999999999999978},\n{'id': 3, 'leaf': '11', 'similarity': 0.5385719658738707}]]\n</code></pre>"},{"location":"api/trees/TfIdf/#methods","title":"Methods","text":"call <p>Search for the closest embedding.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>k     (int)     \u2013 defaults to <code>100</code> </li> <li>k_leafs     (int)     \u2013 defaults to <code>1</code> </li> <li>leafs     (list[int] | None)     \u2013 defaults to <code>None</code> </li> <li>score_documents     (bool)     \u2013 defaults to <code>True</code> </li> <li>beam_search_depth     (int)     \u2013 defaults to <code>1</code> </li> <li>queries_embeddings     (torch.Tensor | numpy.ndarray | dict)     \u2013 defaults to <code>None</code> </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> </ul> add <p>Add documents to the tree.</p> <p>Parameters</p> <ul> <li>documents     (list)    </li> <li>documents_embeddings     (numpy.ndarray | scipy.sparse._csr.csr_matrix | dict)     \u2013 defaults to <code>None</code> </li> <li>k     (int)     \u2013 defaults to <code>1</code> </li> <li>documents_to_leafs     (dict)     \u2013 defaults to <code>None</code> </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> </ul> add_module <p>Adds a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> apply <p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).</p> <p>Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Returns an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> children <p>Returns an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> cpu <p>Moves all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Moves all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> empty <p>Empty the tree.</p> eval <p>Sets the module in evaluation mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> extra_repr <p>Set the extra representation of the module</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.  .. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> <p>Parameters</p> <ul> <li>input     (Any)    </li> </ul> get_buffer <p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_documents_leafs <p>Returns mapping between documents ids and leafs and vice versa.</p> get_extra_state <p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.</p> <p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_mapping_leafs <p>Returns mapping between leafs and their number.</p> get_negative_samples <p>Return negative samples build from the tree.</p> get_parameter <p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_parent <p>Get parent nodes of a specifc node.</p> <p>Parameters</p> <ul> <li>node_name     (int | str)    </li> </ul> get_paths <p>Map leafs to their nodes.</p> get_submodule <p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> ipu <p>Moves all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> load_state_dict <p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p> <p>Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * missing_keys is a list of str containing the missing keys         * unexpected_keys is a list of str containing the unexpected keys  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (Mapping[str, Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> </ul> loss <p>Computes the loss of the tree given the input batch.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>documents     (list[dict])    </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> </ul> modules <p>Returns an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> named_buffers <p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[Set[ForwardRef('Module')]])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> nodes <p>Iterate over the nodes of the tree.</p> <p>Parameters</p> <ul> <li>node     (neural_tree.nodes.node.Node | neural_tree.leafs.leaf.Leaf)     \u2013 defaults to <code>None</code> </li> </ul> parameters <p>Return the parameters of the tree.</p> paths register_backward_hook <p>Registers a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Adds a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Registers a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Registers a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Registers a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Registers a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; Tensor or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Registers a post hook to be run after module's <code>load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Adds a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_pre_hook <p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> set_extra_state <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p> <p>Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code></p> state_dict <p>Returns a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> to <p>Moves and/or casts the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> to_empty <p>Moves the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[str, torch.device])    </li> </ul> to_json <p>Return the tree as a graph.</p> train <p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Moves all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> zero_grad <p>Sets gradients of all model parameters to zero. See similar function under :class:<code>torch.optim.Optimizer</code> for more context.</p> <p>Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/trees/Tree/","title":"Tree","text":"<p>Tree based index for information retrieval.</p>"},{"location":"api/trees/Tree/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> </li> <li> <p>scoring (scoring.SentenceTransformer | scoring.TfIdf)</p> </li> <li> <p>documents (list)</p> </li> <li> <p>leaf_balance_factor (int)</p> </li> <li> <p>branch_balance_factor (int)</p> </li> <li> <p>device</p> </li> <li> <p>seed (int)</p> </li> <li> <p>max_iter (int)</p> </li> <li> <p>n_init (int)</p> </li> <li> <p>n_jobs (int)</p> </li> <li> <p>batch_size (int) \u2013 defaults to <code>None</code></p> </li> <li> <p>create_retrievers (bool) \u2013 defaults to <code>True</code></p> </li> <li> <p>graph (dict | None) \u2013 defaults to <code>None</code></p> </li> <li> <p>documents_embeddings (dict | None) \u2013 defaults to <code>None</code></p> </li> </ul>"},{"location":"api/trees/Tree/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_tree import trees, scoring, clustering\n&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; device = \"cpu\"\n&gt;&gt;&gt; queries = [\n...     \"Paris is the capital of France.\",\n...     \"Berlin\",\n...     \"Berlin\",\n...     \"Paris is the capital of France.\"\n... ]\n&gt;&gt;&gt; documents = [\n...     {\"id\": 0, \"text\": \"Paris is the capital of France.\"},\n...     {\"id\": 1, \"text\": \"Berlin is the capital of Germany.\"},\n...     {\"id\": 2, \"text\": \"Paris and Berlin are European cities.\"},\n...     {\"id\": 3, \"text\": \"Paris and Berlin are beautiful cities.\"},\n... ]\n&gt;&gt;&gt; tree = trees.Tree(\n...    key=\"id\",\n...    documents=documents,\n...    scoring=scoring.TfIdf(key=\"id\", on=[\"text\"], documents=documents),\n...    leaf_balance_factor=1,\n...    branch_balance_factor=2,\n...    device=device,\n...    n_jobs=1,\n... )\n&gt;&gt;&gt; print(tree)\nnode 1\nnode 10\nleaf 100\nleaf 101\nnode 11\nleaf 110\nleaf 111\n&gt;&gt;&gt; tree.documents_to_leafs\n{0: ['100'], 1: ['101'], 2: ['110'], 3: ['111']}\n&gt;&gt;&gt; tree.leafs_to_documents\n{'100': [0], '101': [1], '110': [2], '111': [3]}\n&gt;&gt;&gt; candidates = tree(\n...    queries=queries,\n...    k=2,\n...    k_leafs=2,\n... )\n&gt;&gt;&gt; pprint(candidates[\"documents\"])\n[[{'id': 0, 'leaf': '100', 'similarity': 0.9999999999999978},\n{'id': 1, 'leaf': '101', 'similarity': 0.39941742405759667}],\n[{'id': 3, 'leaf': '111', 'similarity': 0.3523828592933607},\n{'id': 2, 'leaf': '110', 'similarity': 0.348413283355546}],\n[{'id': 3, 'leaf': '111', 'similarity': 0.3523828592933607},\n{'id': 2, 'leaf': '110', 'similarity': 0.348413283355546}],\n[{'id': 0, 'leaf': '100', 'similarity': 0.9999999999999978},\n{'id': 1, 'leaf': '101', 'similarity': 0.39941742405759667}]]\n&gt;&gt;&gt; pprint(candidates[\"tree_scores\"])\n[{'10': tensor(1.0000),\n'100': tensor(1.0000),\n'101': tensor(0.6385),\n'11': tensor(0.1076)},\n{'10': tensor(0.3235),\n'11': tensor(0.3327),\n'110': tensor(0.3327),\n'111': tensor(0.3327)},\n{'10': tensor(0.3235),\n'11': tensor(0.3327),\n'110': tensor(0.3327),\n'111': tensor(0.3327)},\n{'10': tensor(1.0000),\n'100': tensor(1.0000),\n'101': tensor(0.6385),\n'11': tensor(0.1076)}]\n&gt;&gt;&gt; candidates = tree(\n...    queries=queries,\n...    leafs=[\"110\", \"111\", \"111\", \"111\"],\n... )\n&gt;&gt;&gt; pprint(candidates[\"documents\"])\n[[{'id': 2, 'leaf': '110', 'similarity': 0.1036216271728989}],\n[{'id': 3, 'leaf': '111', 'similarity': 0.3523828592933607}],\n[{'id': 3, 'leaf': '111', 'similarity': 0.3523828592933607}],\n[{'id': 3, 'leaf': '111', 'similarity': 0.09981163726061484}]]\n&gt;&gt;&gt; optimizer = torch.optim.AdamW(lr=3e-5, params=list(tree.parameters()))\n&gt;&gt;&gt; loss = tree.loss(\n...    queries=queries,\n...    documents=documents,\n... )\n&gt;&gt;&gt; loss.backward()\n&gt;&gt;&gt; optimizer.step()\n&gt;&gt;&gt; assert loss.item() &gt; 0\n&gt;&gt;&gt; graph = tree.to_json()\n&gt;&gt;&gt; pprint(graph)\n{1: {'10': {'100': [{'id': 0}], '101': [{'id': 1}]},\n'11': {'110': [{'id': 2}], '111': [{'id': 3}]}}}\n&gt;&gt;&gt; graph = {'sport': {'football': {'bayern': [{'id': 2, 'text': 'bayern football team'}],\n...             'psg': [{'id': 1, 'text': 'psg football team'}]},\n...    'rugby': {'toulouse': [{'id': 3, 'text': 'toulouse rugby team'}],\n...              'ville rose': [{'id': 3, 'text': 'toulouse rugby team'},\n...                             {'id': 4, 'text': 'tfc football team'}]}}}\n&gt;&gt;&gt; documents = clustering.get_mapping_nodes_documents(graph=graph)\n&gt;&gt;&gt; tree = trees.Tree(\n...    key=\"id\",\n...    documents=documents,\n...    scoring=scoring.TfIdf(key=\"id\", on=[\"text\"], documents=documents),\n...    leaf_balance_factor=1,\n...    branch_balance_factor=2,\n...    device=device,\n...    graph=graph,\n...    n_jobs=1,\n... )\n&gt;&gt;&gt; tree.documents_to_leafs\n{3: ['ville rose', 'toulouse'], 4: ['ville rose'], 2: ['bayern'], 1: ['psg']}\n&gt;&gt;&gt; tree.leafs_to_documents\n{'ville rose': [3, 4], 'toulouse': [3], 'bayern': [2], 'psg': [1]}\n&gt;&gt;&gt; print(tree)\nnode sport\nnode rugby\nleaf ville rose\nleaf toulouse\nnode football\nleaf bayern\nleaf psg\n&gt;&gt;&gt; candidates = tree(\n...    queries=[\"psg\", \"toulouse\"],\n...    k=2,\n...    k_leafs=2,\n... )\n&gt;&gt;&gt; pprint(candidates[\"documents\"])\n[[{'id': 1, 'leaf': 'psg', 'similarity': 0.5255159378077358}],\n[{'id': 3, 'leaf': 'ville rose', 'similarity': 0.7865788511708137},\n{'id': 3, 'leaf': 'toulouse', 'similarity': 0.7865788511708137}]]\n</code></pre>"},{"location":"api/trees/Tree/#methods","title":"Methods","text":"call <p>Search for the closest embedding.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>k     (int)     \u2013 defaults to <code>100</code> </li> <li>k_leafs     (int)     \u2013 defaults to <code>1</code> </li> <li>leafs     (list[int] | None)     \u2013 defaults to <code>None</code> </li> <li>score_documents     (bool)     \u2013 defaults to <code>True</code> </li> <li>beam_search_depth     (int)     \u2013 defaults to <code>1</code> </li> <li>queries_embeddings     (torch.Tensor | numpy.ndarray | dict)     \u2013 defaults to <code>None</code> </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> </ul> add <p>Add documents to the tree.</p> <p>Parameters</p> <ul> <li>documents     (list)    </li> <li>documents_embeddings     (numpy.ndarray | scipy.sparse._csr.csr_matrix | dict)     \u2013 defaults to <code>None</code> </li> <li>k     (int)     \u2013 defaults to <code>1</code> </li> <li>documents_to_leafs     (dict)     \u2013 defaults to <code>None</code> </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> <li>tqdm_bar     (bool)     \u2013 defaults to <code>True</code> </li> </ul> add_module <p>Adds a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> apply <p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).</p> <p>Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Returns an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> children <p>Returns an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> cpu <p>Moves all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Moves all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> empty <p>Empty the tree.</p> eval <p>Sets the module in evaluation mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> extra_repr <p>Set the extra representation of the module</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.  .. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> <p>Parameters</p> <ul> <li>input     (Any)    </li> </ul> get_buffer <p>Returns the buffer given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_documents_leafs <p>Returns mapping between documents ids and leafs and vice versa.</p> get_extra_state <p>Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.</p> <p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_mapping_leafs <p>Returns mapping between leafs and their number.</p> get_negative_samples <p>Return negative samples build from the tree.</p> get_parameter <p>Returns the parameter given by <code>target</code> if it exists, otherwise throws an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_parent <p>Get parent nodes of a specifc node.</p> <p>Parameters</p> <ul> <li>node_name     (int | str)    </li> </ul> get_paths <p>Map leafs to their nodes.</p> get_submodule <p>Returns the submodule given by <code>target</code> if it exists, otherwise throws an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> ipu <p>Moves all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> load_state_dict <p>Copies parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p> <p>Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * missing_keys is a list of str containing the missing keys         * unexpected_keys is a list of str containing the unexpected keys  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (Mapping[str, Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> </ul> loss <p>Computes the loss of the tree given the input batch.</p> <p>Parameters</p> <ul> <li>queries     (list[str])    </li> <li>documents     (list[dict])    </li> <li>batch_size     (int)     \u2013 defaults to <code>32</code> </li> </ul> modules <p>Returns an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> named_buffers <p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[Set[ForwardRef('Module')]])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> nodes <p>Iterate over the nodes of the tree.</p> <p>Parameters</p> <ul> <li>node     (neural_tree.nodes.node.Node | neural_tree.leafs.leaf.Leaf)     \u2013 defaults to <code>None</code> </li> </ul> parameters <p>Return the parameters of the tree.</p> paths register_backward_hook <p>Registers a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Adds a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Registers a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Registers a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Registers a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Registers a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; Tensor or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Registers a post hook to be run after module's <code>load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Adds a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_pre_hook <p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>, and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> set_extra_state <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p> <p>Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code></p> state_dict <p>Returns a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> to <p>Moves and/or casts the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> to_empty <p>Moves the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[str, torch.device])    </li> </ul> to_json <p>Return the tree as a graph.</p> train <p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Moves all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> zero_grad <p>Sets gradients of all model parameters to zero. See similar function under :class:<code>torch.optim.Optimizer</code> for more context.</p> <p>Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/trees/Tree/#references","title":"References","text":"<p>Li et al., 2023</p>"},{"location":"api/utils/batchify/","title":"batchify","text":""},{"location":"api/utils/batchify/#parameters","title":"Parameters","text":"<ul> <li> <p>X (list[str])</p> </li> <li> <p>batch_size (int)</p> </li> <li> <p>desc (str) \u2013 defaults to ``</p> </li> <li> <p>tqdm_bar (bool) \u2013 defaults to <code>True</code></p> </li> </ul>"},{"location":"api/utils/evaluate/","title":"evaluate","text":"<p>Evaluate candidates matchs.</p>"},{"location":"api/utils/evaluate/#parameters","title":"Parameters","text":"<ul> <li> <p>scores (list[list[dict]])</p> </li> <li> <p>qrels (dict)</p> <p>Qrels.</p> </li> <li> <p>queries_ids (list[str])</p> </li> <li> <p>metrics (list) \u2013 defaults to <code>[]</code></p> <p>Metrics to compute.</p> </li> <li> <p>key (str) \u2013 defaults to <code>id</code></p> </li> </ul>"},{"location":"api/utils/evaluate/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from neural_cherche import models, retrieve, utils\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; _ = torch.manual_seed(42)\n&gt;&gt;&gt; model = models.Splade(\n...     model_name_or_path=\"distilbert-base-uncased\",\n...     device=\"cpu\",\n... )\n&gt;&gt;&gt; documents, queries_ids, queries, qrels = utils.load_beir(\n...     \"scifact\",\n...     split=\"test\",\n... )\n&gt;&gt;&gt; documents = documents[:10]\n&gt;&gt;&gt; retriever = retrieve.Splade(\n...     key=\"id\",\n...     on=[\"title\", \"text\"],\n...     model=model\n... )\n&gt;&gt;&gt; documents_embeddings = retriever.encode_documents(\n...     documents=documents,\n...     batch_size=1,\n... )\n&gt;&gt;&gt; documents_embeddings = retriever.add(\n...     documents_embeddings=documents_embeddings,\n... )\n&gt;&gt;&gt; queries_embeddings = retriever.encode_queries(\n...     queries=queries,\n...     batch_size=1,\n... )\n&gt;&gt;&gt; scores = retriever(\n...     queries_embeddings=queries_embeddings,\n...     k=30,\n...     batch_size=1,\n... )\n&gt;&gt;&gt; utils.evaluate(\n...     scores=scores,\n...     qrels=qrels,\n...     queries_ids=queries_ids,\n...     metrics=[\"map\", \"ndcg@10\", \"ndcg@100\", \"recall@10\", \"recall@100\"]\n... )\n{'map': 0.0033333333333333335, 'ndcg@10': 0.0033333333333333335, 'ndcg@100': 0.0033333333333333335, 'recall@10': 0.0033333333333333335, 'recall@100': 0.0033333333333333335}\n</code></pre>"},{"location":"api/utils/iter/","title":"iter","text":"<p>Iterate over the dataset.</p>"},{"location":"api/utils/iter/#parameters","title":"Parameters","text":"<ul> <li> <p>queries</p> <p>List of queries paired with documents.</p> </li> <li> <p>documents</p> <p>List of documents paired with queries.</p> </li> <li> <p>batch_size \u2013 defaults to <code>512</code></p> <p>Size of the batch.</p> </li> <li> <p>epochs (int) \u2013 defaults to <code>1</code></p> <p>Number of epochs.</p> </li> <li> <p>shuffle \u2013 defaults to <code>True</code></p> </li> <li> <p>tqdm_bar \u2013 defaults to <code>True</code></p> </li> </ul>"},{"location":"api/utils/leafs-precision/","title":"leafs_precision","text":"<p>Calculate the precision of the leafs.</p>"},{"location":"api/utils/leafs-precision/#parameters","title":"Parameters","text":"<ul> <li> <p>key (str)</p> </li> <li> <p>documents (list)</p> </li> <li> <p>leafs (numpy.ndarray)</p> </li> <li> <p>documents_to_leaf (dict)</p> </li> </ul>"},{"location":"api/utils/sanity-check/","title":"sanity_check","text":"<p>Check if the input is valid.</p>"},{"location":"api/utils/sanity-check/#parameters","title":"Parameters","text":"<ul> <li> <p>branch_balance_factor (int)</p> </li> <li> <p>leaf_balance_factor (int)</p> </li> <li> <p>graph (dict)</p> </li> <li> <p>documents (list)</p> </li> </ul>"},{"location":"api/utils/set-env/","title":"set_env","text":"<p>Set environment variables.</p>"},{"location":"evaluate/evaluate/","title":"Evaluate","text":"<p>Neural-tree evaluation is based on RANX. We can also download datasets of BEIR Benchmark with the <code>utils.load_beir</code> function.</p>"},{"location":"evaluate/evaluate/#installation","title":"Installation","text":"<pre><code>pip install \"neural-tree[eval]\"\n</code></pre>"},{"location":"evaluate/evaluate/#usage","title":"Usage","text":"<p>Here is an example of how to train a tree-based index using the <code>scifact</code> dataset and how to evaluate it.</p> <pre><code>import torch\nfrom neural_cherche import models\nfrom sentence_transformers import SentenceTransformer\nfrom neural_tree import clustering, datasets, trees, utils\ndocuments, train_queries, train_documents = datasets.load_beir_train(\ndataset_name=\"scifact\",\n)\nmodel = models.ColBERT(\nmodel_name_or_path=\"raphaelsty/neural-cherche-colbert\",\ndevice=\"cuda\",\n)\n# We intialize a ColBERT index from a\n# SentenceTransformer-based hierarchical clustering.\ntree = trees.ColBERT(\nkey=\"id\",\non=[\"title\", \"text\"],\nmodel=model,\nsentence_transformer=SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\"),\ndocuments=documents,\nleaf_balance_factor=100,\nbranch_balance_factor=5,\nn_jobs=-1,\ndevice=\"cuda\",\nfaiss_device=\"cuda\",\n)\noptimizer = torch.optim.AdamW(lr=3e-3, params=list(tree.parameters()))\nfor step, batch_queries, batch_documents in utils.iter(\nqueries=train_queries,\ndocuments=train_documents,\nshuffle=True,\nepochs=50,\nbatch_size=128,\n):\nloss = tree.loss(\nqueries=batch_queries,\ndocuments=batch_documents,\n)\nloss.backward()\noptimizer.step()\noptimizer.zero_grad(set_to_none=True)\ndocuments, queries_ids, test_queries, qrels = datasets.load_beir_test(\ndataset_name=\"scifact\",\n)\ndocuments_to_leafs = clustering.optimize_leafs(\ntree=tree,\nqueries=train_queries + test_queries,\ndocuments=documents,\n)\ntree = tree.add(\ndocuments=documents,\ndocuments_to_leafs=documents_to_leafs,\n)\ncandidates = tree(\nqueries=test_queries,\nk_leafs=2,  # number of leafs to search\nk=10,  # number of documents to retrieve\n)\ndocuments, queries_ids, test_queries, qrels = datasets.load_beir_test(\ndataset_name=\"scifact\",\n)\ncandidates = tree(\nqueries=test_queries,\nk_leafs=2,\nk=10,\n)\nscores = utils.evaluate(\nscores=candidates[\"documents\"],\nqrels=qrels,\nqueries_ids=queries_ids,\n)\nprint(scores)\n</code></pre> <pre><code>{\"ndcg@10\": 0.6957728027724698, \"hits@1\": 0.59, \"hits@2\": 0.69, \"hits@3\": 0.76, \"hits@4\": 0.8133333333333334, \"hits@5\": 0.8533333333333334, \"hits@10\": 0.91}\n</code></pre>"},{"location":"evaluate/evaluate/#evaluation-dataset","title":"Evaluation dataset","text":"<p>Here are what documents should looks like (an id with multiples fields):</p> <pre><code>[\n{\n\"id\": \"document_0\",\n\"title\": \"Bayesian measures of model complexity and fit\",\n\"text\": \"Summary. We consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure pD for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general pD approximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the \u2018hat\u2019 matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding pD to the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis.\",\n},\n{\n\"id\": \"document_1\",\n\"title\": \"Simplifying likelihood ratios\",\n\"text\": \"Likelihood ratios are one of the best measures of diagnostic accuracy, although they are seldom used, because interpreting them requires a calculator to convert back and forth between \u201cprobability\u201d and \u201codds\u201d of disease. This article describes a simpler method of interpreting likelihood ratios, one that avoids calculators, nomograms, and conversions to \u201codds\u201d of disease. Several examples illustrate how the clinician can use this method to refine diagnostic decisions at the bedside.\",\n},\n]\n</code></pre> <p>Queries is a list of strings:</p> <pre><code>[\n\"Varenicline monotherapy is more effective after 12 weeks of treatment compared to combination nicotine replacement therapies with varenicline or bupropion.\",\n\"Venules have a larger lumen diameter than arterioles.\",\n\"Venules have a thinner or absent smooth layer compared to arterioles.\",\n\"Vitamin D deficiency effects the term of delivery.\",\n\"Vitamin D deficiency is unrelated to birth weight.\",\n\"Women with a higher birth weight are more likely to develop breast cancer later in life.\",\n]\n</code></pre> <p>QueriesIds is a list of ids with respect to the order of queries:</p> <pre><code>[\n\"0\",\n\"1\",\n\"2\",\n\"3\",\n\"4\",\n\"5\",\n]\n</code></pre> <p>Qrels is the mapping between queries ids as key and dict of relevant documents with 1 as value:</p> <pre><code>{\n\"1\": {\"document_0\": 1},\n\"3\": {\"document_10\": 1},\n\"5\": {\"document_5\": 1},\n\"13\": {\"document_22\": 1},\n\"36\": {\"document_23\": 1, \"document_0\": 1},\n\"42\": {\"document_2\": 1},\n}\n</code></pre>"},{"location":"evaluate/evaluate/#metrics","title":"Metrics","text":"<p>We can evaluate our model with various metrics detailed here.</p>"},{"location":"existing_tree/existing_tree/","title":"Build an index from an existing tree","text":"<p>Neural-Tree can build a tree from an existing graph. This is useful when we have a specific use case where we want to retrieve the right leaf for a query. </p> <p>The tree we want to pass should follow some rules:</p> <ul> <li> <p>We should avoid nodes with a lot of children. The more children a node has, the more time it will take to explore this node.</p> </li> <li> <p>A node must have only one parent. This is a rule for the tree to be a tree. You can somehow duplicate a node to have it in multiple places in the tree.</p> </li> </ul> <p>Let's create a tree which has one root node, two children nodes and two leafs nodes which contains up to 3 documents.</p> <pre><code>graph = {\n\"root\": {\n\"science\": {\n\"machine learning\": [\n{\"id\": 0, \"content\": \"bayern football team\"},\n{\"id\": 1, \"content\": \"toulouse rugby team\"},\n],\n\"computer\": [\n{\"id\": 2, \"content\": \"Apple Macintosh\"},\n{\"id\": 3, \"content\": \"Microsoft Windows\"},\n{\"id\": 4, \"content\": \"Linux Ubuntu\"},\n],\n},\n\"history\": {\n\"france\": [\n{\"id\": 5, \"content\": \"history of france\"},\n{\"id\": 6, \"content\": \"french revolution\"},\n],\n\"italia\": [\n{\"id\": 7, \"content\": \"history of rome\"},\n{\"id\": 8, \"content\": \"history of venice\"},\n],\n},\n}\n}\n</code></pre> <p>We can now initialize either a TfIdf, a SentenceTransformer or a ColBERT tree using the graph we have created.</p> <pre><code>from neural_tree import trees\nfrom neural_cherche import models\nmodel = models.ColBERT(\nmodel_name_or_path=\"raphaelsty/neural-cherche-colbert\",\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n)\ntree = trees.ColBERT(\nkey=\"id\",  \non=[\"content\"],\nmodel=model,\ngraph=graph,\nn_jobs=-1, \n)\nprint(tree)\n</code></pre> <p>This will output:</p> <pre><code> node root\nnode science\nleaf computer\nleaf machine learning\nnode history\nleaf france\nleaf italia\n</code></pre> <p>Once we have created our tree we can export it back to json using the <code>tree.to_json()</code>:</p> <pre><code>{\n\"root\": {\n\"science\": {\n\"computer\": [{\"id\": 2}, {\"id\": 3}, {\"id\": 4}],\n\"machine learning\": [{\"id\": 0}, {\"id\": 1}],\n},\n\"history\": {\"france\": [{\"id\": 5}, {\"id\": 6}], \"italia\": [{\"id\": 7}, {\"id\": 8}]},\n}\n}\n</code></pre>"},{"location":"scripts/","title":"Index","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"This script is responsible for building the API reference. The API reference is located in\ndocs/api. The script scans through all the modules, classes, and functions. It processes\nthe __doc__ of each object and formats it so that MkDocs can process it in turn.\n\"\"\"\nimport functools\nimport importlib\nimport inspect\nimport os\nimport pathlib\nimport re\nimport shutil\n</pre> \"\"\"This script is responsible for building the API reference. The API reference is located in docs/api. The script scans through all the modules, classes, and functions. It processes the __doc__ of each object and formats it so that MkDocs can process it in turn. \"\"\" import functools import importlib import inspect import os import pathlib import re import shutil In\u00a0[\u00a0]: Copied! <pre>from numpydoc.docscrape import ClassDoc, FunctionDoc\n</pre> from numpydoc.docscrape import ClassDoc, FunctionDoc In\u00a0[\u00a0]: Copied! <pre>package = \"neural_tree\"\n</pre> package = \"neural_tree\" <p>shutil.copy(\"README.md\", \"docs/index.md\")</p> In\u00a0[\u00a0]: Copied! <pre>def paragraph(text):\n    return f\"{text}\\n\"\n</pre> def paragraph(text):     return f\"{text}\\n\" In\u00a0[\u00a0]: Copied! <pre>def h1(text):\n    return paragraph(f\"# {text}\")\n</pre> def h1(text):     return paragraph(f\"# {text}\") In\u00a0[\u00a0]: Copied! <pre>def h2(text):\n    return paragraph(f\"## {text}\")\n</pre> def h2(text):     return paragraph(f\"## {text}\") In\u00a0[\u00a0]: Copied! <pre>def h3(text):\n    return paragraph(f\"### {text}\")\n</pre> def h3(text):     return paragraph(f\"### {text}\") In\u00a0[\u00a0]: Copied! <pre>def h4(text):\n    return paragraph(f\"#### {text}\")\n</pre> def h4(text):     return paragraph(f\"#### {text}\") In\u00a0[\u00a0]: Copied! <pre>def link(caption, href):\n    return f\"[{caption}]({href})\"\n</pre> def link(caption, href):     return f\"[{caption}]({href})\" In\u00a0[\u00a0]: Copied! <pre>def code(text):\n    return f\"`{text}`\"\n</pre> def code(text):     return f\"`{text}`\" In\u00a0[\u00a0]: Copied! <pre>def li(text):\n    return f\"- {text}\\n\"\n</pre> def li(text):     return f\"- {text}\\n\" In\u00a0[\u00a0]: Copied! <pre>def snake_to_kebab(text):\n    return text.replace(\"_\", \"-\")\n</pre> def snake_to_kebab(text):     return text.replace(\"_\", \"-\") In\u00a0[\u00a0]: Copied! <pre>def inherit_docstring(c, meth):\n\"\"\"Since Python 3.5, inspect.getdoc is supposed to return the docstring from a parent class\n    if a class has none. However this doesn't seem to work for Cython classes.\n    \"\"\"\n\n    doc = None\n\n    for ancestor in inspect.getmro(c):\n        try:\n            ancestor_meth = getattr(ancestor, meth)\n        except AttributeError:\n            break\n        doc = inspect.getdoc(ancestor_meth)\n        if doc:\n            break\n\n    return doc\n</pre> def inherit_docstring(c, meth):     \"\"\"Since Python 3.5, inspect.getdoc is supposed to return the docstring from a parent class     if a class has none. However this doesn't seem to work for Cython classes.     \"\"\"      doc = None      for ancestor in inspect.getmro(c):         try:             ancestor_meth = getattr(ancestor, meth)         except AttributeError:             break         doc = inspect.getdoc(ancestor_meth)         if doc:             break      return doc In\u00a0[\u00a0]: Copied! <pre>def inherit_signature(c, method_name):\n    m = getattr(c, method_name)\n    sig = inspect.signature(m)\n\n    params = []\n\n    for param in sig.parameters.values():\n        if param.name == \"self\" or param.annotation is not param.empty:\n            params.append(param)\n            continue\n\n        for ancestor in inspect.getmro(c):\n            try:\n                ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))\n            except AttributeError:\n                break\n            try:\n                ancestor_param = ancestor_meth.parameters[param.name]\n            except KeyError:\n                break\n            if ancestor_param.annotation is not param.empty:\n                param = param.replace(annotation=ancestor_param.annotation)\n                break\n\n        params.append(param)\n\n    return_annotation = sig.return_annotation\n    if return_annotation is inspect._empty:\n        for ancestor in inspect.getmro(c):\n            try:\n                ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))\n            except AttributeError:\n                break\n            if ancestor_meth.return_annotation is not inspect._empty:\n                return_annotation = ancestor_meth.return_annotation\n                break\n\n    return sig.replace(parameters=params, return_annotation=return_annotation)\n</pre> def inherit_signature(c, method_name):     m = getattr(c, method_name)     sig = inspect.signature(m)      params = []      for param in sig.parameters.values():         if param.name == \"self\" or param.annotation is not param.empty:             params.append(param)             continue          for ancestor in inspect.getmro(c):             try:                 ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))             except AttributeError:                 break             try:                 ancestor_param = ancestor_meth.parameters[param.name]             except KeyError:                 break             if ancestor_param.annotation is not param.empty:                 param = param.replace(annotation=ancestor_param.annotation)                 break          params.append(param)      return_annotation = sig.return_annotation     if return_annotation is inspect._empty:         for ancestor in inspect.getmro(c):             try:                 ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))             except AttributeError:                 break             if ancestor_meth.return_annotation is not inspect._empty:                 return_annotation = ancestor_meth.return_annotation                 break      return sig.replace(parameters=params, return_annotation=return_annotation) In\u00a0[\u00a0]: Copied! <pre>def snake_to_kebab(snake: str) -&gt; str:\n    return snake.replace(\"_\", \"-\")\n</pre> def snake_to_kebab(snake: str) -&gt; str:     return snake.replace(\"_\", \"-\") In\u00a0[\u00a0]: Copied! <pre>def pascal_to_kebab(string):\n    string = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1-\\2\", string)\n    string = re.sub(\"(.)([0-9]+)\", r\"\\1-\\2\", string)\n    return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1-\\2\", string).lower()\n</pre> def pascal_to_kebab(string):     string = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1-\\2\", string)     string = re.sub(\"(.)([0-9]+)\", r\"\\1-\\2\", string)     return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1-\\2\", string).lower() In\u00a0[\u00a0]: Copied! <pre>class Linkifier:\n    def __init__(self):\n        path_index = {}\n        name_index = {}\n\n        modules = {\n            module: importlib.import_module(f\"{package}.{module}\")\n            for module in importlib.import_module(f\"{package}\").__all__\n        }\n\n        def index_module(mod_name, mod, path):\n            path = os.path.join(path, mod_name)\n            dotted_path = path.replace(\"/\", \".\")\n\n            for func_name, func in inspect.getmembers(mod, inspect.isfunction):\n                for e in (\n                    f\"{mod_name}.{func_name}\",\n                    f\"{dotted_path}.{func_name}\",\n                    f\"{func.__module__}.{func_name}\",\n                ):\n                    path_index[e] = os.path.join(path, snake_to_kebab(func_name))\n                    name_index[e] = f\"{dotted_path}.{func_name}\"\n\n            for klass_name, klass in inspect.getmembers(mod, inspect.isclass):\n                for e in (\n                    f\"{mod_name}.{klass_name}\",\n                    f\"{dotted_path}.{klass_name}\",\n                    f\"{klass.__module__}.{klass_name}\",\n                ):\n                    path_index[e] = os.path.join(path, klass_name)\n                    name_index[e] = f\"{dotted_path}.{klass_name}\"\n\n            for submod_name, submod in inspect.getmembers(mod, inspect.ismodule):\n                if submod_name not in mod.__all__ or submod_name == \"typing\":\n                    continue\n                for e in (f\"{mod_name}.{submod_name}\", f\"{dotted_path}.{submod_name}\"):\n                    path_index[e] = os.path.join(path, snake_to_kebab(submod_name))\n\n                # Recurse\n                index_module(submod_name, submod, path=path)\n\n        for mod_name, mod in modules.items():\n            index_module(mod_name, mod, path=\"\")\n\n        # Prepend {package} to each index entry\n        for k in list(path_index.keys()):\n            path_index[f\"{package}.{k}\"] = path_index[k]\n        for k in list(name_index.keys()):\n            name_index[f\"{package}.{k}\"] = name_index[k]\n\n        self.path_index = path_index\n        self.name_index = name_index\n\n    def linkify(self, text, use_fences, depth):\n        path = self.path_index.get(text)\n        name = self.name_index.get(text)\n        if path and name:\n            backwards = \"../\" * (depth + 1)\n            if use_fences:\n                return f\"[`{name}`]({backwards}{path})\"\n            return f\"[{name}]({backwards}{path})\"\n        return None\n\n    def linkify_fences(self, text, depth):\n        between_fences = re.compile(\"`[\\w\\.]+\\.\\w+`\")\n        return between_fences.sub(\n            lambda x: self.linkify(x.group().strip(\"`\"), True, depth) or x.group(), text\n        )\n\n    def linkify_dotted(self, text, depth):\n        dotted = re.compile(\"\\w+\\.[\\.\\w]+\")\n        return dotted.sub(\n            lambda x: self.linkify(x.group(), False, depth) or x.group(), text\n        )\n</pre> class Linkifier:     def __init__(self):         path_index = {}         name_index = {}          modules = {             module: importlib.import_module(f\"{package}.{module}\")             for module in importlib.import_module(f\"{package}\").__all__         }          def index_module(mod_name, mod, path):             path = os.path.join(path, mod_name)             dotted_path = path.replace(\"/\", \".\")              for func_name, func in inspect.getmembers(mod, inspect.isfunction):                 for e in (                     f\"{mod_name}.{func_name}\",                     f\"{dotted_path}.{func_name}\",                     f\"{func.__module__}.{func_name}\",                 ):                     path_index[e] = os.path.join(path, snake_to_kebab(func_name))                     name_index[e] = f\"{dotted_path}.{func_name}\"              for klass_name, klass in inspect.getmembers(mod, inspect.isclass):                 for e in (                     f\"{mod_name}.{klass_name}\",                     f\"{dotted_path}.{klass_name}\",                     f\"{klass.__module__}.{klass_name}\",                 ):                     path_index[e] = os.path.join(path, klass_name)                     name_index[e] = f\"{dotted_path}.{klass_name}\"              for submod_name, submod in inspect.getmembers(mod, inspect.ismodule):                 if submod_name not in mod.__all__ or submod_name == \"typing\":                     continue                 for e in (f\"{mod_name}.{submod_name}\", f\"{dotted_path}.{submod_name}\"):                     path_index[e] = os.path.join(path, snake_to_kebab(submod_name))                  # Recurse                 index_module(submod_name, submod, path=path)          for mod_name, mod in modules.items():             index_module(mod_name, mod, path=\"\")          # Prepend {package} to each index entry         for k in list(path_index.keys()):             path_index[f\"{package}.{k}\"] = path_index[k]         for k in list(name_index.keys()):             name_index[f\"{package}.{k}\"] = name_index[k]          self.path_index = path_index         self.name_index = name_index      def linkify(self, text, use_fences, depth):         path = self.path_index.get(text)         name = self.name_index.get(text)         if path and name:             backwards = \"../\" * (depth + 1)             if use_fences:                 return f\"[`{name}`]({backwards}{path})\"             return f\"[{name}]({backwards}{path})\"         return None      def linkify_fences(self, text, depth):         between_fences = re.compile(\"`[\\w\\.]+\\.\\w+`\")         return between_fences.sub(             lambda x: self.linkify(x.group().strip(\"`\"), True, depth) or x.group(), text         )      def linkify_dotted(self, text, depth):         dotted = re.compile(\"\\w+\\.[\\.\\w]+\")         return dotted.sub(             lambda x: self.linkify(x.group(), False, depth) or x.group(), text         ) In\u00a0[\u00a0]: Copied! <pre>def concat_lines(lines):\n    return inspect.cleandoc(\" \".join(\"\\n\\n\" if line == \"\" else line for line in lines))\n</pre> def concat_lines(lines):     return inspect.cleandoc(\" \".join(\"\\n\\n\" if line == \"\" else line for line in lines)) In\u00a0[\u00a0]: Copied! <pre>def print_docstring(obj, file, depth):\n\"\"\"Prints a classes's docstring to a file.\"\"\"\n\n    doc = ClassDoc(obj) if inspect.isclass(obj) else FunctionDoc(obj)\n\n    printf = functools.partial(print, file=file)\n\n    printf(h1(obj.__name__))\n    printf(linkifier.linkify_fences(paragraph(concat_lines(doc[\"Summary\"])), depth))\n    printf(\n        linkifier.linkify_fences(\n            paragraph(concat_lines(doc[\"Extended Summary\"])), depth\n        )\n    )\n\n    # We infer the type annotations from the signatures, and therefore rely on the signature\n    # instead of the docstring for documenting parameters\n    try:\n        signature = inspect.signature(obj)\n    except ValueError:\n        signature = (\n            inspect.Signature()\n        )  # TODO: this is necessary for Cython classes, but it's not correct\n    params_desc = {param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]}\n\n    # Parameters\n    if signature.parameters:\n        printf(h2(\"Parameters\"))\n    for param in signature.parameters.values():\n        # Name\n        printf(f\"- **{param.name}**\", end=\"\")\n        # Type annotation\n        if param.annotation is not param.empty:\n            anno = inspect.formatannotation(param.annotation)\n            anno = linkifier.linkify_dotted(anno, depth)\n            printf(f\" (*{anno}*)\", end=\"\")\n        # Default value\n        if param.default is not param.empty:\n            printf(f\" \u2013 defaults to `{param.default}`\", end=\"\")\n        printf(\"\\n\", file=file)\n        # Description\n        if param.name in params_desc:\n            desc = params_desc[param.name]\n            if desc:\n                printf(f\"    {desc}\\n\")\n    printf(\"\")\n\n    # Attributes\n    if doc[\"Attributes\"]:\n        printf(h2(\"Attributes\"))\n    for attr in doc[\"Attributes\"]:\n        # Name\n        printf(f\"- **{attr.name}**\", end=\"\")\n        # Type annotation\n        if attr.type:\n            printf(f\" (*{attr.type}*)\", end=\"\")\n        printf(\"\\n\", file=file)\n        # Description\n        desc = \" \".join(attr.desc)\n        if desc:\n            printf(f\"    {desc}\\n\")\n    printf(\"\")\n\n    # Examples\n    if doc[\"Examples\"]:\n        printf(h2(\"Examples\"))\n\n        in_code = False\n        after_space = False\n\n        for line in inspect.cleandoc(\"\\n\".join(doc[\"Examples\"])).splitlines():\n            if (\n                in_code\n                and after_space\n                and line\n                and not line.startswith(\"&gt;&gt;&gt;\")\n                and not line.startswith(\"...\")\n            ):\n                printf(\"```\\n\")\n                in_code = False\n                after_space = False\n\n            if not in_code and line.startswith(\"&gt;&gt;&gt;\"):\n                printf(\"```python\")\n                in_code = True\n\n            after_space = False\n            if not line:\n                after_space = True\n\n            printf(line)\n\n        if in_code:\n            printf(\"```\")\n    printf(\"\")\n\n    # Methods\n    if inspect.isclass(obj) and doc[\"Methods\"]:\n        printf(h2(\"Methods\"))\n        printf_indent = lambda x, **kwargs: printf(f\"    {x}\", **kwargs)\n\n        for meth in doc[\"Methods\"]:\n            printf(paragraph(f'???- note \"{meth.name}\"'))\n\n            # Parse method docstring\n            docstring = inherit_docstring(c=obj, meth=meth.name)\n            if not docstring:\n                continue\n            meth_doc = FunctionDoc(func=None, doc=docstring)\n\n            printf_indent(paragraph(\" \".join(meth_doc[\"Summary\"])))\n            if meth_doc[\"Extended Summary\"]:\n                printf_indent(paragraph(\" \".join(meth_doc[\"Extended Summary\"])))\n\n            # We infer the type annotations from the signatures, and therefore rely on the signature\n            # instead of the docstring for documenting parameters\n            signature = inherit_signature(obj, meth.name)\n            params_desc = {\n                param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]\n            }\n\n            # Parameters\n            if (\n                len(signature.parameters) &gt; 1\n            ):  # signature is never empty, but self doesn't count\n                printf_indent(\"**Parameters**\\n\")\n            for param in signature.parameters.values():\n                if param.name == \"self\":\n                    continue\n                # Name\n                printf_indent(f\"- **{param.name}**\", end=\"\")\n                # Type annotation\n                if param.annotation is not param.empty:\n                    printf_indent(\n                        f\" (*{inspect.formatannotation(param.annotation)}*)\", end=\"\"\n                    )\n                # Default value\n                if param.default is not param.empty:\n                    printf_indent(f\" \u2013 defaults to `{param.default}`\", end=\"\")\n                printf_indent(\"\", file=file)\n                # Description\n                desc = params_desc.get(param.name)\n                if desc:\n                    printf_indent(f\"    {desc}\")\n            printf_indent(\"\")\n\n            # Returns\n            if meth_doc[\"Returns\"]:\n                printf_indent(\"**Returns**\\n\")\n                return_val = meth_doc[\"Returns\"][0]\n                if signature.return_annotation is not inspect._empty:\n                    if inspect.isclass(signature.return_annotation):\n                        printf_indent(\n                            f\"*{signature.return_annotation.__name__}*: \", end=\"\"\n                        )\n                    else:\n                        printf_indent(f\"*{signature.return_annotation}*: \", end=\"\")\n                printf_indent(return_val.type)\n                printf_indent(\"\")\n\n    # Notes\n    if doc[\"Notes\"]:\n        printf(h2(\"Notes\"))\n        printf(paragraph(\"\\n\".join(doc[\"Notes\"])))\n\n    # References\n    if doc[\"References\"]:\n        printf(h2(\"References\"))\n        printf(paragraph(\"\\n\".join(doc[\"References\"])))\n</pre> def print_docstring(obj, file, depth):     \"\"\"Prints a classes's docstring to a file.\"\"\"      doc = ClassDoc(obj) if inspect.isclass(obj) else FunctionDoc(obj)      printf = functools.partial(print, file=file)      printf(h1(obj.__name__))     printf(linkifier.linkify_fences(paragraph(concat_lines(doc[\"Summary\"])), depth))     printf(         linkifier.linkify_fences(             paragraph(concat_lines(doc[\"Extended Summary\"])), depth         )     )      # We infer the type annotations from the signatures, and therefore rely on the signature     # instead of the docstring for documenting parameters     try:         signature = inspect.signature(obj)     except ValueError:         signature = (             inspect.Signature()         )  # TODO: this is necessary for Cython classes, but it's not correct     params_desc = {param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]}      # Parameters     if signature.parameters:         printf(h2(\"Parameters\"))     for param in signature.parameters.values():         # Name         printf(f\"- **{param.name}**\", end=\"\")         # Type annotation         if param.annotation is not param.empty:             anno = inspect.formatannotation(param.annotation)             anno = linkifier.linkify_dotted(anno, depth)             printf(f\" (*{anno}*)\", end=\"\")         # Default value         if param.default is not param.empty:             printf(f\" \u2013 defaults to `{param.default}`\", end=\"\")         printf(\"\\n\", file=file)         # Description         if param.name in params_desc:             desc = params_desc[param.name]             if desc:                 printf(f\"    {desc}\\n\")     printf(\"\")      # Attributes     if doc[\"Attributes\"]:         printf(h2(\"Attributes\"))     for attr in doc[\"Attributes\"]:         # Name         printf(f\"- **{attr.name}**\", end=\"\")         # Type annotation         if attr.type:             printf(f\" (*{attr.type}*)\", end=\"\")         printf(\"\\n\", file=file)         # Description         desc = \" \".join(attr.desc)         if desc:             printf(f\"    {desc}\\n\")     printf(\"\")      # Examples     if doc[\"Examples\"]:         printf(h2(\"Examples\"))          in_code = False         after_space = False          for line in inspect.cleandoc(\"\\n\".join(doc[\"Examples\"])).splitlines():             if (                 in_code                 and after_space                 and line                 and not line.startswith(\"&gt;&gt;&gt;\")                 and not line.startswith(\"...\")             ):                 printf(\"```\\n\")                 in_code = False                 after_space = False              if not in_code and line.startswith(\"&gt;&gt;&gt;\"):                 printf(\"```python\")                 in_code = True              after_space = False             if not line:                 after_space = True              printf(line)          if in_code:             printf(\"```\")     printf(\"\")      # Methods     if inspect.isclass(obj) and doc[\"Methods\"]:         printf(h2(\"Methods\"))         printf_indent = lambda x, **kwargs: printf(f\"    {x}\", **kwargs)          for meth in doc[\"Methods\"]:             printf(paragraph(f'???- note \"{meth.name}\"'))              # Parse method docstring             docstring = inherit_docstring(c=obj, meth=meth.name)             if not docstring:                 continue             meth_doc = FunctionDoc(func=None, doc=docstring)              printf_indent(paragraph(\" \".join(meth_doc[\"Summary\"])))             if meth_doc[\"Extended Summary\"]:                 printf_indent(paragraph(\" \".join(meth_doc[\"Extended Summary\"])))              # We infer the type annotations from the signatures, and therefore rely on the signature             # instead of the docstring for documenting parameters             signature = inherit_signature(obj, meth.name)             params_desc = {                 param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]             }              # Parameters             if (                 len(signature.parameters) &gt; 1             ):  # signature is never empty, but self doesn't count                 printf_indent(\"**Parameters**\\n\")             for param in signature.parameters.values():                 if param.name == \"self\":                     continue                 # Name                 printf_indent(f\"- **{param.name}**\", end=\"\")                 # Type annotation                 if param.annotation is not param.empty:                     printf_indent(                         f\" (*{inspect.formatannotation(param.annotation)}*)\", end=\"\"                     )                 # Default value                 if param.default is not param.empty:                     printf_indent(f\" \u2013 defaults to `{param.default}`\", end=\"\")                 printf_indent(\"\", file=file)                 # Description                 desc = params_desc.get(param.name)                 if desc:                     printf_indent(f\"    {desc}\")             printf_indent(\"\")              # Returns             if meth_doc[\"Returns\"]:                 printf_indent(\"**Returns**\\n\")                 return_val = meth_doc[\"Returns\"][0]                 if signature.return_annotation is not inspect._empty:                     if inspect.isclass(signature.return_annotation):                         printf_indent(                             f\"*{signature.return_annotation.__name__}*: \", end=\"\"                         )                     else:                         printf_indent(f\"*{signature.return_annotation}*: \", end=\"\")                 printf_indent(return_val.type)                 printf_indent(\"\")      # Notes     if doc[\"Notes\"]:         printf(h2(\"Notes\"))         printf(paragraph(\"\\n\".join(doc[\"Notes\"])))      # References     if doc[\"References\"]:         printf(h2(\"References\"))         printf(paragraph(\"\\n\".join(doc[\"References\"]))) In\u00a0[\u00a0]: Copied! <pre>def print_module(mod, path, overview, is_submodule=False):\n    mod_name = mod.__name__.split(\".\")[-1]\n\n    # Create a directory for the module\n    mod_slug = snake_to_kebab(mod_name)\n    mod_path = path.joinpath(mod_slug)\n    mod_short_path = str(mod_path).replace(\"docs/api/\", \"\")\n    os.makedirs(mod_path, exist_ok=True)\n    with open(mod_path.joinpath(\".pages\"), \"w\") as f:\n        f.write(f\"title: {mod_name}\")\n\n    # Add the module to the overview\n    if is_submodule:\n        print(h3(mod_name), file=overview)\n    else:\n        print(h2(mod_name), file=overview)\n    if mod.__doc__:\n        print(paragraph(mod.__doc__), file=overview)\n\n    # Extract all public classes and functions\n    ispublic = lambda x: x.__name__ in mod.__all__ and not x.__name__.startswith(\"_\")\n    classes = inspect.getmembers(mod, lambda x: inspect.isclass(x) and ispublic(x))\n    funcs = inspect.getmembers(mod, lambda x: inspect.isfunction(x) and ispublic(x))\n\n    # Classes\n\n    if classes and funcs:\n        print(\"\\n**Classes**\\n\", file=overview)\n\n    for _, c in classes:\n        print(f\"{mod_name}.{c.__name__}\")\n\n        # Add the class to the overview\n        slug = snake_to_kebab(c.__name__)\n        print(\n            li(link(c.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview\n        )\n\n        # Write down the class' docstring\n        with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:\n            print_docstring(obj=c, file=file, depth=mod_short_path.count(\"/\") + 1)\n\n    # Functions\n\n    if classes and funcs:\n        print(\"\\n**Functions**\\n\", file=overview)\n\n    for _, f in funcs:\n        print(f\"{mod_name}.{f.__name__}\")\n\n        # Add the function to the overview\n        slug = snake_to_kebab(f.__name__)\n        print(\n            li(link(f.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview\n        )\n\n        # Write down the function' docstring\n        with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:\n            print_docstring(obj=f, file=file, depth=mod_short_path.count(\".\") + 1)\n\n    # Sub-modules\n    for name, submod in inspect.getmembers(mod, inspect.ismodule):\n        # We only want to go through the public submodules, such as optim.schedulers\n        if (\n            name in (\"tags\", \"typing\", \"inspect\", \"skmultiflow_utils\")\n            or name not in mod.__all__\n            or name.startswith(\"_\")\n        ):\n            continue\n        print_module(mod=submod, path=mod_path, overview=overview, is_submodule=True)\n\n    print(\"\", file=overview)\n</pre> def print_module(mod, path, overview, is_submodule=False):     mod_name = mod.__name__.split(\".\")[-1]      # Create a directory for the module     mod_slug = snake_to_kebab(mod_name)     mod_path = path.joinpath(mod_slug)     mod_short_path = str(mod_path).replace(\"docs/api/\", \"\")     os.makedirs(mod_path, exist_ok=True)     with open(mod_path.joinpath(\".pages\"), \"w\") as f:         f.write(f\"title: {mod_name}\")      # Add the module to the overview     if is_submodule:         print(h3(mod_name), file=overview)     else:         print(h2(mod_name), file=overview)     if mod.__doc__:         print(paragraph(mod.__doc__), file=overview)      # Extract all public classes and functions     ispublic = lambda x: x.__name__ in mod.__all__ and not x.__name__.startswith(\"_\")     classes = inspect.getmembers(mod, lambda x: inspect.isclass(x) and ispublic(x))     funcs = inspect.getmembers(mod, lambda x: inspect.isfunction(x) and ispublic(x))      # Classes      if classes and funcs:         print(\"\\n**Classes**\\n\", file=overview)      for _, c in classes:         print(f\"{mod_name}.{c.__name__}\")          # Add the class to the overview         slug = snake_to_kebab(c.__name__)         print(             li(link(c.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview         )          # Write down the class' docstring         with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:             print_docstring(obj=c, file=file, depth=mod_short_path.count(\"/\") + 1)      # Functions      if classes and funcs:         print(\"\\n**Functions**\\n\", file=overview)      for _, f in funcs:         print(f\"{mod_name}.{f.__name__}\")          # Add the function to the overview         slug = snake_to_kebab(f.__name__)         print(             li(link(f.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview         )          # Write down the function' docstring         with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:             print_docstring(obj=f, file=file, depth=mod_short_path.count(\".\") + 1)      # Sub-modules     for name, submod in inspect.getmembers(mod, inspect.ismodule):         # We only want to go through the public submodules, such as optim.schedulers         if (             name in (\"tags\", \"typing\", \"inspect\", \"skmultiflow_utils\")             or name not in mod.__all__             or name.startswith(\"_\")         ):             continue         print_module(mod=submod, path=mod_path, overview=overview, is_submodule=True)      print(\"\", file=overview) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    api_path = pathlib.Path(\"docs/api\")\n\n    # Create a directory for the API reference\n    shutil.rmtree(api_path, ignore_errors=True)\n    os.makedirs(api_path, exist_ok=True)\n    with open(api_path.joinpath(\".pages\"), \"w\") as f:\n        f.write(\"title: API reference\\narrange:\\n  - overview.md\\n  - ...\\n\")\n\n    overview = open(api_path.joinpath(\"overview.md\"), \"w\")\n    print(h1(\"Overview\"), file=overview)\n\n    linkifier = Linkifier()\n\n    for mod_name, mod in inspect.getmembers(\n        importlib.import_module(f\"{package}\"), inspect.ismodule\n    ):\n        if mod_name.startswith(\"_\"):\n            continue\n        print(mod_name)\n        print_module(mod, path=api_path, overview=overview)\n</pre> if __name__ == \"__main__\":     api_path = pathlib.Path(\"docs/api\")      # Create a directory for the API reference     shutil.rmtree(api_path, ignore_errors=True)     os.makedirs(api_path, exist_ok=True)     with open(api_path.joinpath(\".pages\"), \"w\") as f:         f.write(\"title: API reference\\narrange:\\n  - overview.md\\n  - ...\\n\")      overview = open(api_path.joinpath(\"overview.md\"), \"w\")     print(h1(\"Overview\"), file=overview)      linkifier = Linkifier()      for mod_name, mod in inspect.getmembers(         importlib.import_module(f\"{package}\"), inspect.ismodule     ):         if mod_name.startswith(\"_\"):             continue         print(mod_name)         print_module(mod, path=api_path, overview=overview)"},{"location":"trees/colbert/","title":"Colbert","text":"<p>The ColBERT Tree utilizes hierarchical clustering with either TfIdf sparse vectors or SentenceTransformer embeddings to set up the tree's initial structure.</p> <p>Following the tree's initialization, documents are sorted into specific leaves according to the hierarchical clustering results. Subsequently, the average ColBERT embeddings of the documents are assigned to each node.</p> <p>During a query, the tree employs the ColBERT pre-trained model and its scoring function to identify the most relevant leaf or leaves for effective search results.</p>"},{"location":"trees/colbert/#documents","title":"Documents","text":"<p>To create a tree-based index for ColBERT, we will need to:</p> <ul> <li>Gather the whole set of documents we want to index.</li> <li>Gather queries paired to documents.</li> <li>Sample the training set in order to evaluate the index.</li> </ul> <pre><code># Whole set of documents we want to index.\ndocuments = [\n{\"id\": 0, \"content\": \"paris\"},\n{\"id\": 1, \"content\": \"london\"},\n{\"id\": 2, \"content\": \"berlin\"},\n{\"id\": 3, \"content\": \"rome\"},\n{\"id\": 4, \"content\": \"bordeaux\"},\n{\"id\": 5, \"content\": \"milan\"},    \n]\n# Paired training documents\ntrain_documents = [\n{\"id\": 0, \"content\": \"paris\"},\n{\"id\": 1, \"content\": \"london\"},\n{\"id\": 2, \"content\": \"berlin\"},\n{\"id\": 3, \"content\": \"rome\"},\n]\n# Paired training queries\ntrain_queries = [\n\"paris is the capital of france\",\n\"london is the capital of england\",\n\"berlin is the capital of germany\",\n\"rome is the capital of italy\",\n]\n</code></pre> <p>Let's train the index using the <code>documents</code>, <code>train_queries</code> and <code>train_documents</code> we have gathered.</p> <pre><code>import torch\nfrom neural_cherche import models\nfrom neural_tree import trees, utils\nmodel = models.ColBERT(\nmodel_name_or_path=\"raphaelsty/neural-cherche-colbert\",\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n)\ntree = trees.ColBERT(\nkey=\"id\",  # The field to use as a key for the documents.\non=[\"content\"],  # The fields to use for the model.\nmodel=model,\ndocuments=documents,\nleaf_balance_factor=100,  # Minimum number of documents per leaf.\nbranch_balance_factor=5,  # Number of childs per node.\nn_jobs=-1,  # We want to set it to 1 when using Google Colab.\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n)\noptimizer = torch.optim.AdamW(lr=3e-3, params=list(tree.parameters()))\nfor step, batch_queries, batch_documents in utils.iter(\nqueries=train_queries,\ndocuments=train_documents,\nshuffle=True,\nepochs=50,\nbatch_size=32,\n):\nloss = tree.loss(\nqueries=batch_queries,\ndocuments=batch_documents,\n)\nloss.backward()\noptimizer.step()\noptimizer.zero_grad(set_to_none=True)\n</code></pre> <p>We can already use the <code>tree</code> to search for documents using the <code>tree</code> method.</p> <p>The <code>call</code> method of the tree outputs a dictionary containing several key pieces of information: the retrieved leaves under leafs, the score assigned to each leaf under scores, a record of the explored nodes and leaves along with their scores under tree_scores, and the documents retrieved for each query listed under documents.</p> <pre><code>tree(\nqueries=[\"history\"],\nk=10, # Number of documents to return for each query. \nk_leafs=1, # The number of leafs to return for each query.\n)\n</code></pre> <pre><code>{\n\"leafs\": array([[\"10\"]], dtype=\"&lt;U2\"), # leafs retrieved\n\"scores\": array([[1.79485011]]), # scores for each leaf\n\"tree_scores\": [ # history of nodes and leafs explored with the respective scores\n{\n\"10\": tensor(1.7949),\n\"12\": tensor(1.5722),\n\"14\": tensor(1.5132),\n\"13\": tensor(0.9872),\n\"11\": tensor(0.8582),\n}\n],\n\"documents\": [ # documents retrieved for each query\n[\n{\"id\": 3, \"similarity\": 1.9020360708236694, \"leaf\": \"10\"},\n{\"id\": 2, \"similarity\": 1.5113722085952759, \"leaf\": \"10\"},\n]\n],\n}\n</code></pre> <p>Once we have trained our index, we should further optimize the tree by duplicating some documents in the tree's leafs. This will allow us to have a better recall when searching for documents. We can use the <code>clustering.optimize_leafs</code> method to optimize the tree. We shoud gather as much queries as possible to optimize the tree.</p> <pre><code>from neural_tree import clustering\ntest_queries = [\n\"bordeaux is a city in the west of france\",\n\"milan is a city in the north of italy\",\n]\ndocuments_to_leafs = clustering.optimize_leafs(\ntree=tree,\nqueries=train_queries + test_queries,  # We gather all the queries we have.\ndocuments=documents,  # The whole set of documents we want to index.\n)\ntree = tree.add(\ndocuments=documents,\ndocuments_to_leafs=documents_to_leafs,\n)\n</code></pre> <p>We can now use the optimized <code>tree</code> to search for documents. We can pass one or multiple queries.</p> <pre><code>tree(\nqueries=[\"history\"],\nk=10, # The number of documents to return for each query. \nk_leafs=1, # The number of leafs to return for each query.\n)\n</code></pre>"},{"location":"trees/colbert/#create-a-colbert-tree-from-a-sentence-transformer","title":"Create a ColBERT tree from a Sentence Transformer","text":"<p>By default the ColBERT tree will we create from a hierarchical clustering using TfIdf sparse vectors. We can also create a ColBERT tree from a Sentence Transformer model. This version is superior to the TfIdf version.</p> <pre><code>import torch\nfrom neural_cherche import models\nfrom sentence_transformers import SentenceTransformer\nfrom neural_tree import trees, utils\ndocuments = [\n{\"id\": 0, \"content\": \"paris\"},\n{\"id\": 1, \"content\": \"london\"},\n{\"id\": 2, \"content\": \"berlin\"},\n{\"id\": 3, \"content\": \"rome\"},\n{\"id\": 4, \"content\": \"bordeaux\"},\n{\"id\": 5, \"content\": \"milan\"},    \n]\nmodel = models.ColBERT(\nmodel_name_or_path=\"raphaelsty/neural-cherche-colbert\",\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n)\ntree = trees.ColBERT(\nkey=\"id\", \non=[\"content\"], \nmodel=model,\ndocuments=documents,\nsentence_transformer=SentenceTransformer(\"all-mpnet-base-v2\"), # sentence-transformer used for hierarchical clustering\nleaf_balance_factor=100, \nbranch_balance_factor=5, \nn_jobs=-1,  # We want to set it to 1 when using Google Colab.\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n)\n</code></pre> <p>Note that in the next version of neural-tree we will be able to create a ColBERT tree from an end-to-end ColBERT clustering procedure in order to better preserve to model accuracy.</p>"},{"location":"trees/sentence_transformer/","title":"SentenceTransformer","text":"<p>The SentenceTransformer Tree employs hierarchical clustering on document embeddings to establish its structure. The embedding for each node is determined by averaging the embeddings of the documents associated with its child nodes.</p> <p>After constructing the index, a Faiss index is initialized within every leaf node. This setup facilitates rapid document retrieval upon identifying the appropriate leaf or leaves.</p> <p>To create a tree-based index for Sentence Transformers, we will need to:</p> <ul> <li>Gather the whole set of documents we want to index.</li> <li>Gather queries paired to documents.</li> <li>Sample the training set in order to evaluate the index.</li> </ul>"},{"location":"trees/sentence_transformer/#documents","title":"Documents","text":"<pre><code># Whole set of documents we want to index.\ndocuments = [\n{\"id\": 0, \"content\": \"paris\"},\n{\"id\": 1, \"content\": \"london\"},\n{\"id\": 2, \"content\": \"berlin\"},\n{\"id\": 3, \"content\": \"rome\"},\n{\"id\": 4, \"content\": \"bordeaux\"},\n{\"id\": 5, \"content\": \"milan\"},    \n]\n# Paired training documents\ntrain_documents = [\n{\"id\": 0, \"content\": \"paris\"},\n{\"id\": 1, \"content\": \"london\"},\n{\"id\": 2, \"content\": \"berlin\"},\n{\"id\": 3, \"content\": \"rome\"},\n]\n# Paired training queries\ntrain_queries = [\n\"paris is the capital of france\",\n\"london is the capital of england\",\n\"berlin is the capital of germany\",\n\"rome is the capital of italy\",\n]\n</code></pre>"},{"location":"trees/sentence_transformer/#training","title":"Training","text":"<p>Let's train the index using the <code>documents</code>, <code>train_queries</code> and <code>train_documents</code> we have gathered.</p> <pre><code>import torch\nfrom sentence_transformers import SentenceTransformer\nfrom neural_tree import trees, utils\ntree = trees.SentenceTransformer(\nkey=\"id\",  # The field to use as a key for the documents.\non=[\"content\"],  # The fields to use for the model.\nmodel=SentenceTransformer(\nmodel_name_or_path=\"sentence-transformers/all-mpnet-base-v2\"\n),\ndocuments=documents,\nleaf_balance_factor=min(100, len(documents) // 4),  # Minimum number of documents per leaf. \nbranch_balance_factor=3,  # Number of childs per node.\nn_jobs=-1,  # We want to set it to 1 when using Google Colab.\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n)\noptimizer = torch.optim.AdamW(lr=3e-3, params=list(tree.parameters()))\nfor step, batch_queries, batch_documents in utils.iter(\nqueries=train_queries,\ndocuments=train_documents,\nshuffle=True,\nepochs=50,\nbatch_size=32,\n):\nloss = tree.loss(\nqueries=batch_queries,\ndocuments=batch_documents,\n)\nloss.backward()\noptimizer.step()\noptimizer.zero_grad(set_to_none=True)\n</code></pre> <p>We can already use the <code>tree</code> to search for documents using the <code>tree</code> method.</p> <p>The <code>call</code> method of the tree outputs a dictionary containing several key pieces of information: the retrieved leaves under leafs, the score assigned to each leaf under scores, a record of the explored nodes and leaves along with their scores under tree_scores, and the documents retrieved for each query listed under documents.</p> <pre><code>tree(\nqueries=[\"history\"],\nk=10, # Number of documents to return for each query. \nk_leafs=1, # The number of leafs to return for each query.\n)\n</code></pre> <pre><code>{\n\"leafs\": array([[\"10\"]], dtype=\"&lt;U2\"), # leafs retrieved\n\"scores\": array([[1.79485011]]), # scores for each leaf\n\"tree_scores\": [ # history of nodes and leafs explored with the respective scores\n{\n\"10\": tensor(1.7949),\n\"12\": tensor(1.5722),\n\"14\": tensor(1.5132),\n\"13\": tensor(0.9872),\n\"11\": tensor(0.8582),\n}\n],\n\"documents\": [ # documents retrieved for each query\n[\n{\"id\": 3, \"similarity\": 1.9020360708236694, \"leaf\": \"10\"},\n{\"id\": 2, \"similarity\": 1.5113722085952759, \"leaf\": \"10\"},\n]\n],\n}\n</code></pre>"},{"location":"trees/sentence_transformer/#optimize-the-tree","title":"Optimize the tree","text":"<p>Once we have trained our index, we should further optimize the tree by duplicating some documents in the tree's leafs. This will allow us to have a better recall when searching for documents. We can use the <code>clustering.optimize_leafs</code> method to optimize the tree. We shoud gather as much queries as possible to optimize the tree.</p> <pre><code>from neural_tree import clustering\ntest_queries = [\n\"bordeaux is a city in the west of france\",\n\"milan is a city in the north of italy\",\n]\ndocuments_to_leafs = clustering.optimize_leafs(\ntree=tree,\nqueries=train_queries + test_queries,  # We gather all the queries we have.\ndocuments=documents,  # The whole set of documents we want to index.\n)\ntree = tree.add(\ndocuments=documents,\ndocuments_to_leafs=documents_to_leafs,\n)\n</code></pre> <p>We can now use the optimized <code>tree</code> to search for documents. We can pass one or multiple queries.</p> <pre><code>tree(\nqueries=[\"history\"],\nk=10, # The number of documents to return for each query. \nk_leafs=1, # The number of leafs to return for each query.\n)\n</code></pre>"},{"location":"trees/tfidf/","title":"TfIdf","text":"<p>To create a tree-based index for Sentence Transformers, we will need to:</p> <ul> <li>Gather the whole set of documents we want to index.</li> <li>Gather queries paired to documents.</li> <li>Sample the training set in order to evaluate the index.</li> </ul> <pre><code># Whole set of documents we want to index.\ndocuments = [\n{\"id\": 0, \"content\": \"paris\"},\n{\"id\": 1, \"content\": \"london\"},\n{\"id\": 2, \"content\": \"berlin\"},\n{\"id\": 3, \"content\": \"rome\"},\n{\"id\": 4, \"content\": \"bordeaux\"},\n{\"id\": 5, \"content\": \"milan\"},    \n]\n# Paired training documents\ntrain_documents = [\n{\"id\": 0, \"content\": \"paris\"},\n{\"id\": 1, \"content\": \"london\"},\n{\"id\": 2, \"content\": \"berlin\"},\n{\"id\": 3, \"content\": \"rome\"},\n]\n# Paired training queries\ntrain_queries = [\n\"paris is the capital of france\",\n\"london is the capital of england\",\n\"berlin is the capital of germany\",\n\"rome is the capital of italy\",\n]\n</code></pre> <p>Let's train the index using the <code>documents</code>, <code>train_queries</code> and <code>train_documents</code> we have gathered.</p> <pre><code>import torch\nfrom neural_tree import trees, utils\nfrom sentence_transformers import SentenceTransformer\ntree = trees.TfIdf(\nkey=\"id\",  # The field to use as a key for the documents.\non=[\"title\", \"content\"],  # The fields to use for the model.\ndocuments=documents,\nleaf_balance_factor=100,  # Minimum number of documents per leaf.\nbranch_balance_factor=5,  # Number of childs per node.\nn_jobs=-1,  # We want to set it to 1 when using Google Colab.\n)\noptimizer = torch.optim.AdamW(lr=3e-3, params=list(tree.parameters()))\nfor step, batch_queries, batch_documents in utils.iter(\nqueries=train_queries,\ndocuments=train_documents,\nshuffle=True,\nepochs=50,\nbatch_size=32,\n):\nloss = tree.loss(\nqueries=batch_queries,\ndocuments=batch_documents,\n)\nloss.backward()\noptimizer.step()\noptimizer.zero_grad(set_to_none=True)\n</code></pre> <p>We can already use the <code>tree</code> to search for documents using the <code>tree</code> method.</p> <p>The <code>call</code> method of the tree outputs a dictionary containing several key pieces of information: the retrieved leaves under leafs, the score assigned to each leaf under scores, a record of the explored nodes and leaves along with their scores under tree_scores, and the documents retrieved for each query listed under documents.</p> <pre><code>tree(\nqueries=[\"history\"],\nk=10, # Number of documents to return for each query. \nk_leafs=1, # The number of leafs to return for each query.\n)\n</code></pre> <pre><code>{\n\"leafs\": array([[\"10\"]], dtype=\"&lt;U2\"), # leafs retrieved\n\"scores\": array([[1.79485011]]), # scores for each leaf\n\"tree_scores\": [ # history of nodes and leafs explored with the respective scores\n{\n\"10\": tensor(1.7949),\n\"12\": tensor(1.5722),\n\"14\": tensor(1.5132),\n\"13\": tensor(0.9872),\n\"11\": tensor(0.8582),\n}\n],\n\"documents\": [ # documents retrieved for each query\n[\n{\"id\": 3, \"similarity\": 1.9020360708236694, \"leaf\": \"10\"},\n{\"id\": 2, \"similarity\": 1.5113722085952759, \"leaf\": \"10\"},\n]\n],\n}\n</code></pre> <p>Once we have trained our index, we should further optimize the tree by duplicating some documents in the tree's leafs. This will allow us to have a better recall when searching for documents. We can use the <code>clustering.optimize_leafs</code> method to optimize the tree. We shoud gather as much queries as possible to optimize the tree.</p> <pre><code>from neural_tree import clustering\ntest_queries = [\n\"bordeaux is a city in the west of france\",\n\"milan is a city in the north of italy\",\n]\ndocuments_to_leafs = clustering.optimize_leafs(\ntree=tree,\nqueries=train_queries + test_queries,  # We gather all the queries we have.\ndocuments=documents,  # The whole set of documents we want to index.\n)\ntree = tree.add(\ndocuments=documents,\ndocuments_to_leafs=documents_to_leafs,\n)\n</code></pre> <p>We can now use the optimized <code>tree</code> to search for documents. We can pass one or multiple queries.</p> <pre><code>tree(\nqueries=[\"history\"],\nk=10, # The number of documents to return for each query. \nk_leafs=1, # The number of leafs to return for each query.\n)\n</code></pre>"}]}